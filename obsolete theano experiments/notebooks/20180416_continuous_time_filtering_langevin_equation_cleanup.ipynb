{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import absolute_import\n",
    "from __future__ import with_statement\n",
    "from __future__ import division\n",
    "from __future__ import nested_scopes\n",
    "from __future__ import generators\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "# Load scipy/numpy/matplotlib\n",
    "from   scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "from   pylab import *\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from warnings import warn\n",
    "\n",
    "# Configure figure resolution\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'   ] = 100\n",
    "\n",
    "from izh       import * # Routines for sampling Izhikevich neurons\n",
    "from plot      import * # Misc. plotting routines\n",
    "from glm       import * # GLM fitting\n",
    "from arppglm   import * # Sampling and integration\n",
    "from utilities import * # Other utilities\n",
    "from arguments import * # Argument verification\n",
    "\n",
    "from theano_arppglm import *\n",
    "\n",
    "print('Workspace Initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved features for GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "saved_training_model = scipy.io.loadmat('saved_training_model.mat')\n",
    "K  = np.array(saved_training_model['K'],dtype=dtype)\n",
    "B  = np.array(saved_training_model['B'],dtype=dtype)\n",
    "By = np.array(saved_training_model['By'],dtype=dtype)\n",
    "Bh = np.array(saved_training_model['Bh'],dtype=dtype)\n",
    "A  = np.array(saved_training_model['A'],dtype=dtype)\n",
    "C  = np.array(saved_training_model['C'],dtype=dtype)\n",
    "Y  = np.array(saved_training_model['Y'],dtype=dtype)\n",
    "dt = np.array(saved_training_model['dt'],dtype=dtype)\n",
    "\n",
    "K  = int(scalar(K))\n",
    "N  = prod(Y.shape)\n",
    "Y  = np.squeeze(Y)\n",
    "X  = concatenate([By,Bh],axis=1)\n",
    "\n",
    "# Don't use all training data\n",
    "STARTPLOT = 2500\n",
    "NPLOT = 3000\n",
    "Y  = Y[STARTPLOT:STARTPLOT+NPLOT]\n",
    "By = By[STARTPLOT:STARTPLOT+NPLOT]\n",
    "Bh = Bh[STARTPLOT:STARTPLOT+NPLOT]\n",
    "X  = X[STARTPLOT:STARTPLOT+NPLOT]\n",
    "\n",
    "N = len(X)\n",
    "STARTPLOT=0\n",
    "NPLOT=N\n",
    "print('Saved GLM features loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GLM on CPU and verify that filtering approximates basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def lograte(p):\n",
    "    '''\n",
    "    Log-intensity of point process model on this dataset\n",
    "    Predicted using the standard GLM way\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    lograte = m + Bh.dot(beta_st) + By.dot(beta)\n",
    "    return lograte\n",
    "\n",
    "def logmean(M1,p):\n",
    "    '''\n",
    "    Projected history process\n",
    "    Predicted using history-process means\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    M1      = np.squeeze(M1)\n",
    "    return (beta.T.dot(M1.T))[0] + (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "def get_stim(p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "    return stim\n",
    "\n",
    "def filter_GLM_np(p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = get_stim(p)\n",
    "    allM1_np = np.zeros((N,K))\n",
    "    M1       = np.zeros((K,1))\n",
    "    for i in range(N):\n",
    "        R   = sexp(p0[1:K+1].dot(M1)+m+stim[i])\n",
    "        M1 += A.dot(M1)*dt + C.dot(R)\n",
    "        allM1_np[i] = M1[:,0]\n",
    "    return allM1_np\n",
    "\n",
    "def addspikes():\n",
    "    for t in find(Y>0):\n",
    "        axvline(t,color=OCHRE,lw=0.4)\n",
    "    \n",
    "def niceaxis():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\",message='No labelled objects found')\n",
    "        legend()\n",
    "    simpleraxis()\n",
    "    xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "    addspikes()\n",
    "\n",
    "print('GLM helpers done')\n",
    "\n",
    "# Re-fit GLM\n",
    "m,bhat  = fitGLM(X,Y)\n",
    "\n",
    "# Re-pack model parameters\n",
    "p0      = np.zeros((1+len(bhat)))\n",
    "p0[0 ]  = m\n",
    "p0[1:]  = bhat\n",
    "\n",
    "allM1_np = filter_GLM_np(p0)\n",
    "\n",
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,label='conditional intensity')\n",
    "subplot(311)\n",
    "plot(logmean(allM1_np,p0),lw=0.4,label='mean-field',color=RUST)\n",
    "niceaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Langevin equation\n",
    "\n",
    "Take the original process and add a likelihood potential\n",
    "\n",
    "$$\n",
    "dx = \\left[ \\mu(x,t) - \\nabla \\log P(y|x) \\right] dt + \\sigma(x,t) \\cdot dW\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, illustrate Langevin sampling on Theano of the original point process\n",
    "\n",
    "All the process noise comes from spiking, so for the single neuron case study we juts need a 1D sequence of random values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Theano inputs and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "Xst  = T.matrix(\"Xst\",dtype=dtype) # stimulus history features\n",
    "Ysp  = T.vector(\"Ysp\",dtype=dtype) # spikes\n",
    "par  = T.vector(\"par\",dtype=dtype) # packed parameter vectors\n",
    "\n",
    "# Unpack parameter vector\n",
    "b    = par[1:K+1] # spike history weights\n",
    "bst  = par[K+1:]  # stimulus weights\n",
    "mm   = par[0]     # constant offset\n",
    "\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "\n",
    "# Pre-compute projected stimulus\n",
    "# This evaluates to a vector\n",
    "stim = mm + Xst.dot(bst)\n",
    "\n",
    "# Hard-coded parameters\n",
    "oversample   = 4\n",
    "dt           = 1.0\n",
    "dtf          = dt/oversample\n",
    "maxrate      = 2.0\n",
    "maxlogr      = Tcon(log(maxrate*dt))\n",
    "minlogr      = Tcon(log(sqrt(np.finfo('float32').tiny)*dt))\n",
    "maxlogrf     = Tcon(log(maxrate*dtf))\n",
    "minlogrf     = Tcon(log(sqrt(np.finfo('float32').tiny)*dtf))\n",
    "#maxrate      = Tcon(maxrate)\n",
    "\n",
    "# Cast system operators to theano matrices\n",
    "Aop  = Tcon(A)\n",
    "Cop  = Tcon(C)\n",
    "Adtf = Tcon(A*dtf)\n",
    "F    = T.slinalg.expm(Adtf)\n",
    "\n",
    "print('Parameters and constants defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define random state\n",
    "\n",
    "[\"a random variable is drawn at most once during any single function execution\"](http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "M = 1000 # No. samples to draw\n",
    "# NxM matrix of random numbers\n",
    "G = srng.normal((N,oversample,M))\n",
    "print('Random state defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Theano Langevin sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def GLM_langevin(s,g,z):\n",
    "    # M: No. samples\n",
    "    # K: No. dimensions\n",
    "    # s : scalar stimulus\n",
    "    # g : M-vector of random numbers\n",
    "    # z : list of column vectors, i.e. KxM  matrix\n",
    "    for i in range(oversample):\n",
    "        m  = b.T.dot(z)\n",
    "        lr = s + m + T.log(dtf)\n",
    "        lr = T.clip(lr,minlogrf,maxlogrf) # M-vector\n",
    "        R0 = Tsexp(lr) # M-vector\n",
    "        Y  = g[i]*T.sqrt(R0) + R0 # M-vector\n",
    "        #z += Adtf.dot(z) # deterministic\n",
    "        z = F.dot(z)\n",
    "        z += Cop*(Y) # stochastic\n",
    "    return z,lr # KxM  matrix\n",
    "\n",
    "z0 = Tcon(np.zeros((K,M)))\n",
    "\n",
    "[allz,alll], lup = theano.scan(GLM_langevin,\n",
    "                            sequences     = [stim,G],\n",
    "                            outputs_info  = [z0,None],\n",
    "                            non_sequences = [],\n",
    "                            n_steps       = N,\n",
    "                            name          = 'GLM_langevin')\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "langevin_GLM = Tfun(inp = [Xst,par],\n",
    "                  out = [allz,alll],\n",
    "                  upd = lup)\n",
    "\n",
    "print('Theano Langevin sampler defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare theano sampler to CPU sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from arppglm import langevin_sample_history\n",
    "\n",
    "p = p0.copy()\n",
    "\n",
    "# Sample using Numpy\n",
    "tic()\n",
    "stim_np = get_stim(p)\n",
    "beta_np = ascolumn(p[1:K+1])\n",
    "allZ_np,allL_np = langevin_sample_history(stim_np,A,beta_np,C,\n",
    "                    dt         = dt,\n",
    "                    M          = M,\n",
    "                    maxrate    = maxrate,\n",
    "                    oversample = oversample) \n",
    "toc()\n",
    "\n",
    "# Sample using Theano\n",
    "tic()\n",
    "allZ_th,allL_th = langevin_GLM(Bh,p)\n",
    "toc()\n",
    "\n",
    "subplot(311)\n",
    "#plot(allL_np,lw=0.1,color=BLACK);\n",
    "stderrplot(mean(allL_np,1),var(allL_np,1),color=BLACK,lw=0.2)\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "subplot(312)\n",
    "#plot(allL_th,lw=0.1,color=BLACK);\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.2)\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate potential\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(y|x) = \\nabla[ y \\log \\lambda - \\lambda]\n",
    "= (y - \\lambda) z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y \\nabla (s + \\beta^\\top z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla e^{s+\\beta^\\top z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Hard-coded parameters\n",
    "oversample   = 5\n",
    "dt           = 1.0\n",
    "dtf          = dt/oversample\n",
    "maxrate      = 2.0\n",
    "maxlogr      = Tcon(log(maxrate*dt))\n",
    "minlogr      = Tcon(log(sqrt(np.finfo('float32').tiny)))#*dt))\n",
    "maxlogrf     = Tcon(log(maxrate*dtf))\n",
    "minlogrf     = Tcon(log(sqrt(np.finfo('float32').tiny)))#*dtf))\n",
    "#maxrate      = Tcon(maxrate)\n",
    "\n",
    "# Cast system operators to theano matrices\n",
    "Aop  = Tcon(A)\n",
    "Cop  = Tcon(C)\n",
    "Adtf = Tcon(A*dtf)\n",
    "F    = T.slinalg.expm(Adtf)\n",
    "\n",
    "M = 500 # No. samples to draw\n",
    "# NxM matrix of random numbers\n",
    "G = srng.normal((N,oversample,M))\n",
    "\n",
    "tau   = 2.5\n",
    "alpha = T.exp(-dtf/tau)\n",
    "\n",
    "def GLM_potential(s,y,g,z):\n",
    "    # M : No. samples\n",
    "    # K : No. dimensions\n",
    "    # s : scalar stimulus\n",
    "    # g : M-vector of random numbers\n",
    "    # z : list of column vectors, i.e. KxM  matrix\n",
    "    \n",
    "    #z  = y*z + z\n",
    "    z  = 0.5*y*beta.T + z\n",
    "    \n",
    "    m  = b.T.dot(z)\n",
    "    lr = s + m + T.log(dtf)\n",
    "    lr = T.clip(lr,minlogrf,maxlogrf) # M-vector\n",
    "    R00 = Tsexp(lr) # M-vector\n",
    "    \n",
    "    for i in range(oversample):\n",
    "        m  = b.T.dot(z)\n",
    "        lr = s + m + T.log(dtf)\n",
    "        lr = T.clip(lr,minlogrf,maxlogrf) # M-vector\n",
    "        R0 = Tsexp(lr) # M-vector\n",
    "        Y  = g[i]*T.sqrt(R0) + R0 # M-vector\n",
    "        z  = F.dot(z)\n",
    "        z += Cop*Y\n",
    "        z -= 0.5*R00*beta.T\n",
    "        z -= 0.9*R00*z\n",
    "        #z -= R0*(0.08*beta.T+0.92*z)\n",
    "        #z -= R0*(0.6*beta.T+0.4*z)\n",
    "        \n",
    "    #z  = y*beta.T + z\n",
    "    z  = 0.5*y*beta.T + z\n",
    "        \n",
    "    m  = b.T.dot(z)\n",
    "    lr = s + m + T.log(dt)\n",
    "    lr = T.clip(lr,minlogr,maxlogr) # M-vector\n",
    "    return z,lr # KxM matrix\n",
    "\n",
    "z0 = Tcon(np.random.randn(*(K,M)))\n",
    "\n",
    "[allzp,alllp], pup = theano.scan(GLM_potential,\n",
    "                            sequences     = [stim,Ysp,G],\n",
    "                            outputs_info  = [z0,None],\n",
    "                            non_sequences = [],\n",
    "                            n_steps       = N,\n",
    "                            name          = 'GLM_potential')\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "potential_GLM = Tfun(inp = [Xst,Ysp,par],\n",
    "                  out = [allzp,alllp],\n",
    "                  upd = pup)\n",
    "\n",
    "print('Theano Langevin potential sampler defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "p = p0.copy()\n",
    "\n",
    "# Sample using Numpy moment closure\n",
    "subplot(411)\n",
    "tic()\n",
    "allLR,allLV,allM1,allM2,nll = filter_moments(stim_np,Y,A,beta_np,C,p[0],\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = 100,\n",
    "    method      = \"moment_closure\",\n",
    "#    method      = \"second_order\",\n",
    "#    method      = \"approximate_gamma\",\n",
    "    int_method  = \"exponential\",\n",
    "#    int_method  = \"euler\",\n",
    "    measurement = \"laplace\",\n",
    "    reg_cov     = 0.00000,\n",
    "    reg_rate    = 0)\n",
    "toc()\n",
    "stderrplot(allLR,allLV,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,0)\n",
    "\n",
    "# Sample using Theano\n",
    "subplot(412)\n",
    "tic()\n",
    "allZ_th,allL_th = potential_GLM(Bh,Y,p)\n",
    "toc()\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,0)\n",
    "\n",
    "subplot(413)\n",
    "plot(allLR,lw=0.5,color=AZURE)\n",
    "plot(mean(allL_th,1),lw=0.5,color=BLACK)\n",
    "niceaxis()\n",
    "xlim(1900,2200)\n",
    "\n",
    "subplot(414)\n",
    "plot(exp(allLR)*(1+0.5*allLV),lw=0.5,color=AZURE)\n",
    "plot(mean(exp(allL_th),1),lw=0.5,color=BLACK)\n",
    "niceaxis()\n",
    "ylim(0,1)\n",
    "xlim(1900,2200)\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y1,l1 = ensemble_sample(stim_np,B,beta_np,M)\n",
    "\n",
    "print('Mean rate data    ',mean(Y))\n",
    "print('Mean rate sampled ',mean(y1))\n",
    "print('Mean rate langevin',mean(exp(allL_th)))\n",
    "print('Mean rate moments ',mean(exp(allLR)*(1+0.5*allLV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "y2 = np.random.poisson(exp(allL_th))\n",
    "\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1.T>0),cmap='gray')\n",
    "noaxis()\n",
    "niceaxis()\n",
    "\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2.T>0),cmap='gray')\n",
    "niceaxis()\n",
    "noaxis()\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "srng = RandomStreams(seed=234)\n",
    "M = 200 # No. samples to draw\n",
    "# NxM matrix of random numbers\n",
    "G = srng.normal((N,oversample,M))\n",
    "'''\n",
    "\n",
    "G = Tcon(np.random.randn(N,oversample,M))\n",
    "\n",
    "def GLM_langevin_likelihood(s,y,g,nll,z):\n",
    "    # M: No. samples\n",
    "    # K: No. dimensions\n",
    "    # s : scalar stimulus\n",
    "    # g : M-vector of random numbers\n",
    "    # z : list of column vectors, i.e. KxM  matrix\n",
    "    ll = 0\n",
    "    dy = y*dtf\n",
    "    for i in range(oversample):\n",
    "        m  = b.T.dot(z)\n",
    "        lr = s + m + T.log(dtf)\n",
    "        lr = T.minimum(lr,maxlogrf)\n",
    "        #lr = T.clip(lr,minlogrf,maxlogrf) # M-vector\n",
    "        R0 = Tsexp(lr) # M-vector\n",
    "        ll+= T.mean(dy*lr - R0)\n",
    "        Y  = g[i]*T.sqrt(R0) + R0 # M-vector\n",
    "        z  = F.dot(z)\n",
    "        z += Cop*Y\n",
    "    m  = b.T.dot(z)\n",
    "    lr = s + m + T.log(dt)\n",
    "    lr = T.clip(lr,minlogr,maxlogr) # M-vector\n",
    "    return nll-ll,z,lr # KxM  matrix\n",
    "\n",
    "z0 = Tcon(np.zeros((K,M)))\n",
    "[CNLLLV,allzvl,alllvl], puvl = theano.scan(GLM_langevin_likelihood,\n",
    "                            sequences     = [stim,Ysp,G],\n",
    "                            outputs_info  = [Tcon(0),z0,None],\n",
    "                            non_sequences = [],\n",
    "                            n_steps       = N,\n",
    "                            name          = 'GLM_langevin_likelihood')\n",
    "NLLLV = CNLLLV[-1]\n",
    "# GLM negative log-likelihood and history means\n",
    "NLL_GLM_LV = Tfun(inp = [Xst,Ysp,par],\n",
    "                  out = [NLLLV,allzvl,alllvl],\n",
    "                  upd = puvl)\n",
    "\n",
    "print('Theano Langevin potential likelihood defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p0.copy() + randn(*p0.shape)*1e-6\n",
    "#p1[1:K+1] *= 0.125\n",
    "nllv,_,_ = NLL_GLM_LV(Bh,Y,p1)\n",
    "print(nllv)\n",
    "result_LV = p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "large   = sqrt(np.finfo('float32').max)\n",
    "def objective_LV(p):\n",
    "    nllv,_,_ = NLL_GLM_LV(Bh,Y,p)\n",
    "    nll = nllv\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        nll = large\n",
    "    return nll\n",
    "\n",
    "print('Starting optimization')\n",
    "result_LV = minimize_retry(objective_LV,result_LV,jac=False,verbose=verbose,simplex_only=True,tol=1e-12)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LV])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LV-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare\n",
    "\n",
    "Sampling from GLMfit and the new optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample using Theano\n",
    "subplot(411)\n",
    "tic()\n",
    "allZ_th,allL_th = potential_GLM(Bh,Y,result_LV)\n",
    "toc()\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "# Sample using Theano\n",
    "subplot(412)\n",
    "tic()\n",
    "allZ_th,allL_th = langevin_GLM(Bh,result_LV)\n",
    "toc()\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "y1,l1 = ensemble_sample(stim_np,B,beta_np,M)\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1.T>0),cmap='gray')\n",
    "noaxis()\n",
    "niceaxis()\n",
    "stim2_np = get_stim(result_LV)\n",
    "beta2_np = result_LV[1:K+1]\n",
    "y2,l2 = ensemble_sample(stim2_np,B,beta2_np,M)\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2.T>0),cmap='gray')\n",
    "noaxis()\n",
    "niceaxis()\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "reg_cov = 0.01\n",
    "\n",
    "def GLM_potential_likelihood(s,y,g,nll,z):\n",
    "    # M: No. samples\n",
    "    # K: No. dimensions\n",
    "    # s : scalar stimulus\n",
    "    # g : M-vector of random numbers\n",
    "    # z : list of column vectors, i.e. KxM  matrix\n",
    "    ll = 0\n",
    "    dy = y*dtf\n",
    "    for i in range(oversample):\n",
    "        m  = b.T.dot(z)\n",
    "        lr = s + m + T.log(dtf)\n",
    "        lr = T.clip(lr,minlogrf,maxlogrf) # M-vector\n",
    "        R0 = Tsexp(lr) # M-vector\n",
    "        ll+= T.mean(dy*lr - R0)\n",
    "        Y  = g[i]*T.sqrt(R0) + R0 # M-vector\n",
    "        z  = F.dot(z)\n",
    "        z += Cop*Y\n",
    "        z += (dy-R0)*z\n",
    "    m  = b.T.dot(z)\n",
    "    lr = s + m + T.log(dt)\n",
    "    lr = T.clip(lr,minlogr,maxlogr) # M-vector\n",
    "    return nll-ll,z,lr # KxM  matrix\n",
    "\n",
    "z0 = Tcon(np.zeros((K,M)))\n",
    "[CNLLLP,allzpl,alllpl], pupl = theano.scan(GLM_potential_likelihood,\n",
    "                            sequences     = [stim,Ysp,G],\n",
    "                            outputs_info  = [Tcon(0),z0,None],\n",
    "                            non_sequences = [],\n",
    "                            n_steps       = N,\n",
    "                            name          = 'GLM_potential_likelihood')\n",
    "NLLLP = CNLLLP[-1]\n",
    "# GLM negative log-likelihood and history means\n",
    "NLL_GLM_LP = Tfun(inp = [Xst,Ysp,par],\n",
    "                  out = [NLLLP,allzpl,alllpl],\n",
    "                  upd = pupl)\n",
    "\n",
    "print('Theano Langevin potential likelihood defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_LP = result_LV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "large   = sqrt(np.finfo('float32').max)\n",
    "def objective_LP(p):\n",
    "    nllp,_,_ = NLL_GLM_LP(Bh,Y,p)\n",
    "    nllv,_,_ = NLL_GLM_LV(Bh,Y,p)\n",
    "    nll = nllp + nllv\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        nll = large\n",
    "    return nll\n",
    "\n",
    "print('Starting optimization')\n",
    "result_LP = minimize_retry(objective_LP,result_LP,jac=False,verbose=verbose,simplex_only=True,tol=1e-12)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LV])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LV-p0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample using Theano\n",
    "subplot(411)\n",
    "tic()\n",
    "allZ_th,allL_th = potential_GLM(Bh,Y,result_LP)\n",
    "toc()\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.5,label='With updates')\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "# Sample using Theano\n",
    "subplot(412)\n",
    "tic()\n",
    "nllv,allZ_th,allL_th = NLL_GLM_LV(Bh,Y,result_LP)\n",
    "#allZ_th,allL_th = langevin_GLM(Bh,result_LV)\n",
    "toc()\n",
    "stderrplot(mean(allL_th,1),var(allL_th,1),color=BLACK,lw=0.5,label='No updates')\n",
    "niceaxis()\n",
    "ylim(-35,5)\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "y1,l1 = ensemble_sample(stim_np,B,beta_np,M)\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1.T>0),cmap='gray')\n",
    "noaxis()\n",
    "niceaxis()\n",
    "stim2_np = get_stim(result_LP)\n",
    "beta2_np = result_LP[1:K+1]\n",
    "y2,l2 = ensemble_sample(stim2_np,B,beta2_np,M)\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2.T>0),cmap='gray')\n",
    "noaxis()\n",
    "niceaxis()\n",
    "tight_layout()\n",
    "\n",
    "print('Mean rate         ',mean(Y))\n",
    "print('Mean rate GLMfit  ',mean(y1))\n",
    "print('Mean rate LVfit   ',mean(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
