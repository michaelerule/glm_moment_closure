{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Load scipy/numpy/matplotlib\n",
    "from   scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "from   pylab import *\n",
    "\n",
    "# Configure figure resolution\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'   ] = 100\n",
    "\n",
    "from izh       import * # Routines for sampling Izhikevich neurons\n",
    "from plot      import * # Misc. plotting routines\n",
    "from glm       import * # GLM fitting\n",
    "from arppglm   import * # Sampling and integration\n",
    "from utilities import * # Other utilities\n",
    "from arguments import * # Argument verification\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "dtype = 'float32'\n",
    "\n",
    "import os\n",
    "flags = 'mode=FAST_RUN,device=gpu,floatX=%s'%dtype\n",
    "\n",
    "#flags = 'mode=fast_compile,device=gpu,floatX=%s'%dtype\n",
    "if dtype!='float64':\n",
    "    flags += ',warn_float64=warn'\n",
    "print(flags)\n",
    "os.environ[\"THEANO_FLAGS\"] = flags\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "from theano.compile.nanguardmode import NanGuardMode\n",
    "NANGUARD = NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=True)\n",
    "\n",
    "print('Workspace Initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Theano helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def Tcon(x):\n",
    "    return T.constant(x,dtype=dtype)\n",
    "\n",
    "eps     = 1e-4#Tcon(np.finfo('float32').eps)\n",
    "max_exp = Tcon(4)#Tcon(np.log(np.sqrt(np.finfo('float32').max)))\n",
    "\n",
    "def nozero(x):\n",
    "    '''Clip number to be larger than `eps`'''\n",
    "    return T.maximum(eps,x)\n",
    "    #return T.log(1+T.exp(x*10))/10\n",
    "\n",
    "def Tslog(x):\n",
    "    '''Theano safe logarithm'''\n",
    "    return T.log(nozero(x))\n",
    "\n",
    "def Tsexp(x):\n",
    "    return T.exp(T.minimum(max_exp,x))\n",
    "\n",
    "def Tsinv(x):\n",
    "    return 1.0/nozero(x)\n",
    "\n",
    "def Tsdiv(a,x):\n",
    "    return a/nozero(x)\n",
    "\n",
    "def Tfun(inp=None,out=None,upd=None):\n",
    "    return theano.function(inputs               = inp,\n",
    "                           outputs              = out,\n",
    "                           updates              = upd,\n",
    "                           on_unused_input      = 'warn',\n",
    "                           allow_input_downcast = True)\n",
    "#,\n",
    " #                          mode                 = NANGUARD)\n",
    "    \n",
    "print('Theano helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved features for GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "saved_training_model = scipy.io.loadmat('saved_training_model.mat')\n",
    "K  = np.array(saved_training_model['K'],dtype=dtype)\n",
    "B  = np.array(saved_training_model['B'],dtype=dtype)\n",
    "By = np.array(saved_training_model['By'],dtype=dtype)\n",
    "Bh = np.array(saved_training_model['Bh'],dtype=dtype)\n",
    "A  = np.array(saved_training_model['A'],dtype=dtype)\n",
    "C  = np.array(saved_training_model['C'],dtype=dtype)\n",
    "Y  = np.array(saved_training_model['Y'],dtype=dtype)\n",
    "dt = np.array(saved_training_model['dt'],dtype=dtype)\n",
    "\n",
    "K  = int(scalar(K))\n",
    "N  = prod(Y.shape)\n",
    "Y  = np.squeeze(Y)\n",
    "X  = concatenate([By,Bh],axis=1)\n",
    "\n",
    "# Don't use all training data\n",
    "'''N  = 3000\n",
    "Y  = Y[:N]\n",
    "By = By[:N]\n",
    "Bh = Bh[:N]\n",
    "X  = X[:N]'''\n",
    "\n",
    "STARTPLOT = 3000\n",
    "NPLOT = 2500\n",
    "Y  = Y[STARTPLOT:STARTPLOT+NPLOT]\n",
    "By = By[STARTPLOT:STARTPLOT+NPLOT]\n",
    "Bh = Bh[STARTPLOT:STARTPLOT+NPLOT]\n",
    "X  = X[STARTPLOT:STARTPLOT+NPLOT]\n",
    "\n",
    "N = len(X)\n",
    "STARTPLOT=0\n",
    "NPLOT=N\n",
    "\n",
    "print('Saved GLM features loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GLM on CPU and verify that filtering approximates basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def lograte(p):\n",
    "    '''\n",
    "    Log-intensity of point process model on this dataset\n",
    "    Predicted using the standard GLM way\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    lograte = m + Bh.dot(beta_st) + By.dot(beta)\n",
    "    return lograte\n",
    "\n",
    "def logmean(M1,p):\n",
    "    '''\n",
    "    Projected history process\n",
    "    Predicted using history-process means\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    M1      = np.squeeze(M1)\n",
    "    return (beta.T.dot(M1.T))[0] + (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "def filter_GLM_np(p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "    allM1_np = np.zeros((N,K))\n",
    "    M1       = np.zeros((K,1))\n",
    "    for i in range(N):\n",
    "        R   = sexp(p0[1:K+1].dot(M1)+m+stim[i])\n",
    "        M1 += A.dot(M1)*dt + C.dot(R)\n",
    "        allM1_np[i] = M1[:,0]\n",
    "    return allM1_np\n",
    "\n",
    "def addspikes():\n",
    "    for t in find(Y>0):\n",
    "        axvline(t,color=OCHRE,lw=0.4)\n",
    "    \n",
    "def niceaxis():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\",message='No labelled objects found')\n",
    "        legend()\n",
    "    simpleraxis()\n",
    "    xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "    addspikes()\n",
    "\n",
    "print('GLM helpers done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Re-fit GLM\n",
    "m,bhat  = fitGLM(X,Y)\n",
    "\n",
    "# Re-pack model parameters\n",
    "p0      = np.zeros((1+len(bhat)))\n",
    "p0[0 ]  = m\n",
    "p0[1:]  = bhat\n",
    "\n",
    "allM1_np = filter_GLM_np(p0)\n",
    "\n",
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,label='conditional intensity')\n",
    "subplot(311)\n",
    "plot(logmean(allM1_np,p0),lw=0.4,label='mean-field',color=RUST)\n",
    "niceaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Fit GLM on Theano using mean-field\n",
    "\n",
    "This will neglect conditional structure available in the data and focus on matching the slow-timescales with the mean-field limit of the GLM\n",
    "\n",
    "We should adjust the negative-log-likelihood for best machine precision. It should be rescaled so that the initial guess has negative-log-likelihood of zero, and so that the gradient is steep enough so as not to suffer from precision loss. We will implement this with a scalar offset and gain for the negative log-likelihood, passed as parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "Xst  = T.matrix(\"Xst\",dtype=dtype) # stimulus history features\n",
    "Ysp  = T.vector(\"Ysp\",dtype=dtype) # spikes\n",
    "par  = T.vector(\"par\",dtype=dtype) # packed parameter vectors\n",
    "\n",
    "# Negative log-likelihood gain and offset to\n",
    "# mitigate precision loss\n",
    "llgain = T.scalar('llgain',dtype=dtype)\n",
    "llbias = T.scalar('llbias',dtype=dtype)\n",
    "\n",
    "# Cast A to theano consatn\n",
    "Aop = Tcon(A)\n",
    "Cop = Tcon(C)\n",
    "\n",
    "# Unpack parameter vector\n",
    "b    = par[1:K+1] # spike history weights\n",
    "bst  = par[K+1:]  # stimulus weights\n",
    "mm   = par[0]     # constant offset\n",
    "\n",
    "# Pre-compute projected stimulus\n",
    "# This evaluates to a vector\n",
    "stim = mm + Xst.dot(bst)\n",
    "\n",
    "# Hard-coded parameters\n",
    "oversample   = 5\n",
    "dt           = 1.0\n",
    "maxrate      = Tcon(2)\n",
    "maxlogr      = Tslog(maxrate)\n",
    "\n",
    "# Constants\n",
    "dtf = dt/oversample\n",
    "Adt = Aop*dtf\n",
    "\n",
    "# Initial condition for moments\n",
    "# These should be theano variables as they will \n",
    "# need to be passedto initialize the scan function\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*eps\n",
    "\n",
    "def GLM_ll(y,s,M1):                  # scalar, scalar, column\n",
    "    m      = b.T.dot(M1)             # b:vector, M1:column, result:len-1 vector\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    ll     = y*logr-Tsexp(logr)      # 1 vector\n",
    "    return ll[0]                     # scalar\n",
    "\n",
    "def integrate_GLM(M1,s):             # column, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = Tsexp(logx)           # 1 vector\n",
    "        R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "        M1  += Cop*(R0*dtf)          # Kx1 matrix (column) spiking noise\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    return M1\n",
    "    \n",
    "def GLM_filter(y,s,nll,M1):          # scalar, scalar, scalar, column\n",
    "    M1 = integrate_GLM(M1,s)         # Kx1 matrix (column)\n",
    "    ll = GLM_ll(y,s,M1)              # scalar\n",
    "    ll = ll*llgain+llbias            # scalar\n",
    "    return nll-ll,M1                 # scalar, column\n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_GLM, allM1_GLM], updates_GLM = theano.scan(GLM_filter,\n",
    "                                            sequences     = [Ysp,stim],\n",
    "                                            outputs_info  = [Tcon(0),M1],\n",
    "                                            non_sequences = [],\n",
    "                                            n_steps       = N,\n",
    "                                            name          = 'GLM_updates')\n",
    "\n",
    "sumnll_GLM     = cumnll_GLM[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_GLM = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out = [sumnll_GLM,allM1_GLM],\n",
    "                  upd = updates_GLM)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_GLM    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out = [sumnll_GLM],\n",
    "                  upd = updates_GLM)\n",
    "\n",
    "print('Theano GLM mean-field fitting routines defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for MFfilt GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "ll_gain = 1\n",
    "ll_bias = 0\n",
    "\n",
    "grad_GLM = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                out = [theano.gradient.jacobian(sumnll_GLM,par)],\n",
    "                upd = updates_GLM)\n",
    "print(grad_GLM)\n",
    "\n",
    "'''\n",
    "# Hessian takes way too long to build\n",
    "hess_GLM = Tfun(inp = [Xst,Ysp,par], \n",
    "                out = [theano.gradient.hessian(sumnll_GLM,par)],\n",
    "                upd = updates_GLM)\n",
    "print(hess_GLM)\n",
    "'''\n",
    "\n",
    "print('(gradients done)')\n",
    "\n",
    "### Check numeric gradients\n",
    "\n",
    "from glm import numeric_grad, numeric_hess\n",
    "\n",
    "print(grad_GLM(Bh,Y,p0,ll_gain,ll_bias)[0])\n",
    "print(numeric_grad(lambda x:NLL_GLM(Bh,Y,x,ll_gain,ll_bias),p0))#,np.finfo('float32').eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate suitable log-likelihood gain and bias for best precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def recalibrate_likelihood(NLL_fun,grad_fun,p,verbose=False):\n",
    "    global ll_gain, ll_bias\n",
    "    ll_gain = 1\n",
    "    ll_bias = 0\n",
    "    maxgain = 100\n",
    "    normalize_gain_to = sqrt(N)\n",
    "    nll0   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not isfinite(nll0):\n",
    "        warn('Likelihood is not finite, cannot rescale!')\n",
    "        return\n",
    "    gr0    = grad_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not all(isfinite(gr0)):\n",
    "        warn('Gradient is not finite, cannot rescale!')\n",
    "        return\n",
    "    rmsgr0 = max(1e-6,sqrt(mean(gr0**2)))\n",
    "    if verbose: print('initial nll',nll0)\n",
    "    if verbose: print('initial root mean squared gradient',rmsgr0)\n",
    "    ll_gain = min(maxgain,normalize_gain_to/rmsgr0)\n",
    "    if verbose: print('ll_gain',ll_gain)\n",
    "    gr1    = grad_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not all(isfinite(gr1)):\n",
    "        warn('Gradient is not finite, cannot rescale!')\n",
    "        return\n",
    "    rmsgr1 = max(1e-6,sqrt(mean(gr1**2)))\n",
    "    print('rescaled RMS gradient',rmsgr1)\n",
    "    nll1   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not isfinite(nll1):\n",
    "        warn('Likelihood is not finite, cannot rescale!')\n",
    "        return\n",
    "    if verbose: print('rescaled nll',nll1)\n",
    "    ll_bias = nll1/N\n",
    "    for i in range(5):\n",
    "        nll2   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "        if not isfinite(nll2):\n",
    "            warn('Likelihood is not finite, cannot rescale!')\n",
    "            return\n",
    "        ll_bias += nll2/N\n",
    "    if verbose: print('ll_bias',ll_bias)\n",
    "    print('Shifted nll is ',nll2)\n",
    "\n",
    "recalibrate_likelihood(NLL_GLM,grad_GLM,p0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize GLM on Theano using mean-field filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "result = array(p0).copy()\n",
    "\n",
    "# Begin at previous best guess for this result\n",
    "# result = [-5.17943358722,-26.4301822764,-9.09410987003,9.08387276285,0.472761995485,-2.36125927326,1.31525541432,-1.60600566038,0.0284241341053,0.847363810973,-1.03972611426,1.94527757845,-0.81379858301,0.785027995141,-0.234973129833,-0.103747652871,-0.00791078822163]\n",
    "#result = [-6.45553910662,11.2856391467,-20.3654371892,3.87616060703,-1.88432240077,-2.61043347539,-1.62103766004,0.33936732957,-1.91357319581,1.27403649416,-1.21780969965,2.28911681412,-0.619505705252,1.13827148037,0.0377996712583,-0.31138730526,-0.0608561896186]\n",
    "\n",
    "#result = [-6.79133649847,-29.2860814902,-3.30612421806,-0.906768019782,1.99314064172,-10.2237003882,13.1065486065,\n",
    "#          -11.3484098455,0.361443691536,8.3253484781,-5.50982057309,2.49346127861,0.4618367441,0.00426349208114,\n",
    "#          -0.393253366568,0.512513212603,-0.199505838674]\n",
    "\n",
    "result = [-6.48913576754,-43.636990799,-5.48686452302,0.784435684979,-5.24260149358,-10.2759276512,13.6851897803,-12.8003504345,-3.54568724789,10.7072811055,-6.37363364666,3.69099435923,-1.05639579671,2.31080008374,-1.84169083341,1.26295152888,-0.32236353414]\n",
    "\n",
    "print(result)\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from plot import v2str, v2str_long\n",
    "\n",
    "verbose   = False\n",
    "tolerance = 1e-9\n",
    "maxiter   = 1000\n",
    "maxfev    = 1000\n",
    "\n",
    "def objective_GLM(p):\n",
    "    nll = NLL_GLM(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        raise ArithmeticError('Invalid likelihood')\n",
    "    return nll\n",
    "\n",
    "def gradient_GLM(p):\n",
    "    g = array(grad_GLM(Bh,Y,p,ll_gain,ll_bias)).ravel()\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('∇=',v2str_long(g))\n",
    "    if not all(isfinite(g)):\n",
    "        raise ArithmeticError('Invalid gradient')\n",
    "    return g\n",
    "\n",
    "def og_GLM(p):\n",
    "    nll = objective_GLM(p)\n",
    "    g   = gradient_GLM(p)\n",
    "    return nll,g\n",
    "\n",
    "print(\"Beginning phase 2 (gradient-free)\")\n",
    "recalibrate_likelihood(NLL_GLM,grad_GLM,result)\n",
    "result = minimize_retry(og_GLM,result,jac=True,verbose=verbose,failthrough=True,\n",
    "                        options={'maxiter':1,'maxfev':1})\n",
    "recalibrate_likelihood(NLL_GLM,grad_GLM,result)\n",
    "result = minimize_retry(objective_GLM,result,jac=None,verbose=verbose,failthrough=True,\n",
    "                        options={'maxiter':100,'maxfev':100},tol=tolerance,simplex_only=True)\n",
    "print(\"Finished phase 2 (gradient-free)\")\n",
    "\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that mean-field fitting is close to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result),lw=0.4,label=\"MF-filter\")\n",
    "title('Mean-field fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "NLL,allM1 = filter_GLM(Bh,Y,result,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result),lw=0.4,label=\"MF-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('Mean-field fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "\n",
    "def get_sample(p,M=100):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim_np  = (m + Bh.dot(beta_st))[:,0]\n",
    "    y,l = ensemble_sample(stim_np,B,beta,M=M)\n",
    "    return y,l\n",
    "\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('MFfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('MFfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result)).squeeze()\n",
    "print('Mean-field mean-rate for MFfilt is',mean(rate2))\n",
    "\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('MFfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize GLM using LNA without measurement updates\n",
    "\n",
    "Incorporate estimate of fluctuations into likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field definitions)\n",
    "# Limit variance corrections\n",
    "maxvcorr = 100\n",
    "\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "\n",
    "reg_1 = ateb.dot(T.eye(1)*1e-6).dot(ateb)\n",
    "reg_2 = T.eye(K)*1e-6\n",
    "reg_C = reg_2 + reg_2\n",
    "\n",
    "# Additional constants for covariance evolution\n",
    "Cop = Tcon(C)                        # Matrix\n",
    "Cb  = Cop.dot(beta)                  # Matrix\n",
    "CC  = Cop.dot(Cop.T)                 # Matrix\n",
    "\n",
    "def LNA_ll(y,s,M1,M2):               # scalar, scalar, column, matrix\n",
    "    m      = b.T.dot(M1)             # b:vector, M1:column, result:len-1 vector\n",
    "    v      = b.T.dot(M2).dot(b)      # scalar\n",
    "    v      = nozero(v)\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    vc     = 1+0.5*v\n",
    "    vc     = T.minimum(maxvcorr,vc)  # scalar\n",
    "    R1     = R0*vc                   # 1 vector\n",
    "    ll     = y*logr-R1               # 1 vector\n",
    "    return ll[0]                     # scalar\n",
    "\n",
    "def integrate_LNA(M1,M2,s):          # column, matrix, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = Tsexp(logx)           # 1 vector\n",
    "        R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "        R0  *= dtf                   # 1 vector\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1  += C*(R0)                # Kx1 matrix (column) spiking noise\n",
    "        J    = Cb*R0+Adt             # KxK matrix\n",
    "        JM2  = J.dot(M2)             # KxK matrix\n",
    "        M2  += JM2 + JM2.T           # KxK matrix\n",
    "        M2  += CC*R0                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "def LNA_filter(y,s,nll,M1,M2):       # scalar, scalar, scalar, column, matrix\n",
    "    M1,M2 = integrate_LNA(M1,M2,s)   # Kx1 matrix (column)\n",
    "    ll    = LNA_ll(y,s,M1,M2)        # scalar\n",
    "    ll    = ll*llgain+llbias         # scalar\n",
    "    return nll-ll,M1,M2     # scalar, column, matrix\n",
    "\n",
    "# Initial condition for second moment\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_LNA, allM1_LNA, allM2_LNA], upLNA = theano.scan(LNA_filter,\n",
    "                                                sequences     = [Ysp,stim],\n",
    "                                                outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                non_sequences = [],\n",
    "                                                n_steps       = N,\n",
    "                                                name          = 'LNA_filter')\n",
    "sumnll_LNA = cumnll_LNA[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_LNA = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out = [sumnll_LNA,allM1_LNA, allM2_LNA],\n",
    "                  upd = upLNA)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_LNA    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out = [sumnll_LNA],\n",
    "                  upd = upLNA)\n",
    "\n",
    "print('Defined LNA filtering GLM likelihood (no measurements)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate LNA without measurement updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def logvar(allM1,allM2,pp):\n",
    "    m       = array(pp).ravel()[0]\n",
    "    beta    = ascolumn(pp[1:K+1])\n",
    "    beta_st = ascolumn(pp[1+K:])\n",
    "    return np.array([beta.T.dot(m2).dot(beta) for m2 in allM2]).ravel()\n",
    "\n",
    "subplot(311)\n",
    "nll ,allM1, allM2 = filter_LNA(Bh,Y,p0,ll_gain,ll_bias)\n",
    "lm = logmean(allM1,p0)\n",
    "lv = logvar(allM1,allM2,p0)\n",
    "stderrplot(lm,lv,lw=0.5,label='LNA')\n",
    "niceaxis()\n",
    "ylim(-20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for LNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "grad_LNA = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                out = [theano.gradient.jacobian(sumnll_LNA,par)],\n",
    "                upd = upLNA)\n",
    "print(grad_LNA)\n",
    "print('(done)')\n",
    "\n",
    "recalibrate_likelihood(NLL_LNA,grad_LNA,p0)\n",
    "print(grad_LNA(Bh,Y,p0,ll_gain,ll_bias)[0])\n",
    "print(numeric_grad(lambda x:NLL_LNA(Bh,Y,x,ll_gain,ll_bias),p0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize GLM using LNA without measurement updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "x0 = p0 + randn(*p0.shape)*1e-4\n",
    "result_LNA = x0\n",
    "\n",
    "'''result_LNA = np.float128(['-5.26722577475', '-0.958476224094', '0.105001143274', '-1.3878277659', '-0.0530694634475', \n",
    "              '-1.15022127732', '-0.365875934645', '-0.0831215026589', '-0.50788907261', '-0.27615738965', \n",
    "              '0.721112129138', '-0.108877379667', '0.40652935799',  '-0.0195945097271', '0.174251749772', \n",
    "              '-0.161673313388', '-0.00141694765251'])\n",
    "'''\n",
    "\n",
    "result_LNA = [-5.7632510371,-0.694558984913,-0.365826544528,-4.80686708703,7.4369467517,-9.42028908679,7.119563688,-4.36419864785,0.734860845532,4.10351996955,-4.63602850103,3.91815160103,-1.97827248286,1.0536266355,-0.408518056414,0.141022673197,-0.0571683297285]\n",
    "\n",
    "result_LNA = [-5.9082880366,-0.73193407386,-0.368048019254,-4.72408820063,7.43576560468,-9.46939441666,6.9198757086,-4.36632236796,0.668945083848,4.15128851375,-4.63347162332,3.90207045538,-1.95639635235,1.05582847329,-0.404315514029,0.148024458989,-0.0594086298297]\n",
    "\n",
    "result_LNA = [-5.75926275329,-1.93504555723,0.389645810086,-4.06103323968,4.80955049052,-6.43743677629,4.35937131174,-3.21838662035,0.159402568812,4.44417794461,-4.48033625618,3.5044055644,-1.60090301317,0.993904308549,-0.544158672297,0.303170618085,-0.118989985737]\n",
    "\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "tolerance = 1e-9\n",
    "maxiter   = 1000\n",
    "maxfev    = 1000\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "\n",
    "def objective_LNA(p):\n",
    "    nll = NLL_LNA(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        else:\n",
    "            nll = large\n",
    "    return nll\n",
    "\n",
    "def gradient_LNA(p):\n",
    "    g = array(grad_LNA(Bh,Y,p,ll_gain,ll_bias)).ravel()\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('∇=',v2str_long(g))\n",
    "    if not all(isfinite(g)):\n",
    "        print('x=',v2str_long(p))\n",
    "        print('∇=',v2str_long(g))\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid gradient')\n",
    "        else:\n",
    "            bad = ~isfinite(g)\n",
    "            g[bad] = -large*sign[p[bad]]\n",
    "    return g\n",
    "\n",
    "def og_LNA(p):\n",
    "    nll = objective_LNA(p)\n",
    "    g   = gradient_LNA(p)\n",
    "    return nll,g\n",
    "\n",
    "print('Starting optimization')\n",
    "#recalibrate_likelihood(NLL_LNA,grad_LNA,result_LNA)\n",
    "#result_LNA = minimize_retry(og_LNA,result_LNA,jac=True,verbose=verbose,simplex_only=False)\n",
    "recalibrate_likelihood(NLL_LNA,grad_LNA,result_LNA)\n",
    "result_LNA = minimize_retry(objective_LNA,result_LNA,jac=False,verbose=verbose,simplex_only=True)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LNA])+']')\n",
    "\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LNA-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that LNA filtering regression is close to GLMfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_LNA),lw=0.4,label=\"LNA-filter\")\n",
    "title('LNA fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "NLL,allM1 = filter_GLM(Bh,Y,result_LNA,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_LNA),lw=0.4,label=\"LNA-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('LNA fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_LNA,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('LNAfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('LNAfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_LNA)).squeeze()\n",
    "print('Mean-field mean-rate for LNAfilt is',mean(rate2))\n",
    "\n",
    "figure()\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('LNAfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "nll ,allM1, allM2 = filter_LNA(Bh,Y,result_LNA,ll_gain,ll_bias)\n",
    "lm = logmean(allM1,p0)\n",
    "lv = logvar(allM1,allM2,p0)\n",
    "stderrplot(lm,lv,lw=0.5,label='LNA')\n",
    "niceaxis()\n",
    "ylim(-20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate measurement updates into LNA filtering\n",
    "\n",
    "So far, we have only been computing the likelihood against single-time marginals from mean-field and LNA approximations to the point process. This neglects autocorrelations to some extent. Measurement updates are the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "M2   = Tcon(np.eye(K)*1e-7)\n",
    "Adt  = Tcon(A*dt)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "oversample = 1\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "maxlogr = 2\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,10),dtype='float32') # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*(Tsdiv((x-m)**2,v)+Tslog(v*2*pi)) # vector\n",
    "    lograte = x+s+Tslog(dt)                          # vector\n",
    "    lograte = T.maximum(maxlogr,lograte)             # vector\n",
    "    logPyx  = y*lograte-Tsexp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*Tsexp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = Tsexp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "def gaussian_measurement(y,s,M1,M2): # scalar scalar column matrix\n",
    "    m        = b.T.dot(M1)           # scalar\n",
    "    m        = T.maximum(-30,T.minimum(0,m))\n",
    "    v        = b.T.dot(M2).dot(b)    # scalar\n",
    "    v        = nozero(v)\n",
    "    v        = T.minimum(5,v)\n",
    "    t        = Tsinv(v)              # scalar\n",
    "    mp,vp,ll = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    mp       = T.maximum(-50,T.minimum(0,mp))\n",
    "    vp       = T.minimum(5,vp)\n",
    "    tp       = Tsinv(vp)             # scalar\n",
    "    tr       = nozero(tp-t)          # scalar\n",
    "    vr       = Tsinv(tr)             # scalar\n",
    "    mr       = (mp*tp-m*t)*vr        # scalar\n",
    "    M2b      = M2.dot(beta.T)        # column\n",
    "    K        = Tsdiv(M2b,vr+v)       # column\n",
    "    M2       = M2-K.dot(M2b.T)       # matrix\n",
    "    M1       = M1+K*(mr-m)           # column \n",
    "\n",
    "    # try this instead?\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    vc     = 1+0.5*v\n",
    "    vc     = T.minimum(maxvcorr,vc)  # scalar\n",
    "    R1     = R0*vc                   # 1 vector\n",
    "    ll     = (y*logr-R1)[0]          # scalar\n",
    "    \n",
    "    # Or just this?\n",
    "    #logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    ll     = (y*logr-R0)[0]          # scalar\n",
    "    \n",
    "    return M1,M2,ll                  # column matrix scalar\n",
    "\n",
    "def LNA_filter2(y,s,nll,M1,M2):   \n",
    "    M1,M2    = integrate_LNA(M1,M2,s)\n",
    "    M1,M2,ll = gaussian_measurement(y,s,M1,M2)\n",
    "    ll = ll*llgain+llbias            # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_LNA2, allM1_LNA2, allM2_LNA2], upLNA2 = theano.scan(LNA_filter2,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'LNA_filter2')\n",
    "sumnll_LNA2 = cumnll_LNA2[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_LNA2 = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_LNA2,allM1_LNA2,allM2_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_LNA2    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "\n",
    "print('LNA filter with measurement defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the LNA filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "p1 = p0.copy()\n",
    "p1[1:]*=1\n",
    "\n",
    "nll_LNA2,allM1_LNA2,allM2_LNA2 = filter_LNA2(Bh,Y,p1,1,0)\n",
    "print(nll_LNA2)\n",
    "\n",
    "subplot(211)\n",
    "lm = logmean(allM1_LNA2,p1)\n",
    "lv = logvar (allM1_LNA2,allM2_LNA2,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for LNA filtering with measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "grad_LNA2 = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                out = [theano.gradient.jacobian(sumnll_LNA2,par)],\n",
    "                upd = upLNA2)\n",
    "print(grad_LNA2)\n",
    "print('(done)')\n",
    "\n",
    "recalibrate_likelihood(NLL_LNA2,grad_LNA2,p0)\n",
    "print(grad_LNA2(Bh,Y,p0,ll_gain,ll_bias)[0])\n",
    "print(numeric_grad(lambda x:NLL_LNA2(Bh,Y,x,ll_gain,ll_bias),p0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "x0 = p0 + randn(*p1.shape)*1e-9\n",
    "\n",
    "#result_LNA2 = [-5.13048001067,-415.677466381,-58.6853534913,0.640717102487,0.289196327452,-1.1329032582,1.05792467917,-1.87960737134,0.00188514477745,0.27137708466,-1.00679392865,1.1763385601,-0.885005870205,0.366264407826,-0.0784877198932,-0.0042149027294,0.000947067564686]\n",
    "result_LNA2 = [-6.3711051636,-533.547373746,-63.2488749151,0.671634495526,0.261015707365,-1.05631137572,0.84005698839,-1.90515676106,0.00173034013035,0.252625882765,-0.965267180984,1.88838447593,-0.722298349021,0.315590544524,-0.0893822561899,-0.00383353378557,0.000656080567056]\n",
    "\n",
    "print(result_LNA2)\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "tolerance = 1e-9\n",
    "maxiter   = 1000\n",
    "maxfev    = 1000\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "\n",
    "def objective_LNA2(p):\n",
    "    nll = NLL_LNA2(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        else:\n",
    "            nll = large\n",
    "    return nll\n",
    "\n",
    "def gradient_LNA2(p):\n",
    "    g = array(grad_LNA2(Bh,Y,p,ll_gain,ll_bias)).ravel()\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('∇=',v2str_long(g))\n",
    "    if not all(isfinite(g)):\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid gradient')\n",
    "        else:\n",
    "            bad = ~isfinite(g)\n",
    "            g[bad] = -large*sign(p[bad])\n",
    "    return g\n",
    "\n",
    "def og_LNA2(p):\n",
    "    nll = objective_LNA2(p)\n",
    "    g   = gradient_LNA2(p)\n",
    "    return nll,g\n",
    "\n",
    "print('Starting optimization')\n",
    "recalibrate_likelihood(NLL_LNA2,grad_LNA2,result_LNA2)\n",
    "result_LNA2 = minimize_retry(objective_LNA2,result_LNA2,jac=False,verbose=verbose,simplex_only=True)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LNA2])+']')\n",
    "\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LNA2-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_LNA2),lw=0.4,label=\"LNA-filter\")\n",
    "title('LNA fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "nll_LNA2,allM1_LNA2,allM2_LNA2 = filter_LNA2(Bh,Y,result_LNA2,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_LNA2),lw=0.4,label=\"LNA-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('LNA fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_LNA2,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('LNAfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('LNAfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_LNA2)).squeeze()\n",
    "print('Mean-field mean-rate for LNAfilt is',mean(rate2))\n",
    "\n",
    "figure()\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('LNAfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-order moment closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,10),dtype='float32') # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*(Tsdiv((x-m)**2,v)+Tslog(v*2*pi)) # vector\n",
    "    lograte = x+s+Tslog(dt)                          # vector\n",
    "    lograte = T.maximum(maxlogr,lograte)             # vector\n",
    "    logPyx  = y*lograte-Tsexp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*Tsexp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = Tsexp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "def gaussian_measurement(y,s,M1,M2,dt): # scalar scalar column matrix\n",
    "    m        = b.T.dot(M1)           # scalar\n",
    "    m        = T.clip(m,-30,0)       # scalar\n",
    "    v        = b.T.dot(M2).dot(b)    # scalar\n",
    "    v        = T.clip(v,1e-3,5)      # scalar\n",
    "    t        = Tsinv(v)              # scalar\n",
    "    mp,vp,ll = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    mp       = T.clip(mp,-30,0)\n",
    "    vp       = T.clip(vp,1e-3,5)\n",
    "    tp       = Tsinv(vp)             # scalar\n",
    "    tr       = nozero(tp-t)          # scalar\n",
    "    vr       = Tsinv(tr)             # scalar\n",
    "    mr       = (mp*tp-m*t)*vr        # scalar\n",
    "    M2b      = M2.dot(beta.T)        # column\n",
    "    K        = Tsdiv(M2b,vr+v)       # column\n",
    "    M2       = M2-K.dot(M2b.T)       # matrix\n",
    "    M1       = M1+K*(mr-m)           # column \n",
    "    '''\n",
    "    # try this instead?\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    vc     = 1+0.5*v\n",
    "    vc     = T.minimum(maxvcorr,vc)  # scalar\n",
    "    R1     = R0*vc                   # 1 vector\n",
    "    ll     = (y*logr-R1)[0]          # scalar\n",
    "    '''\n",
    "    # Or just this?\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    ll     = (y*logr-R0)[0]          # scalar\n",
    "    \n",
    "    return M1,M2,ll                  # column matrix scalar\n",
    "\n",
    "def integrate_2ND(M1,M2,s):          # column, matrix, scalar\n",
    "    logx = b.T.dot(M1)+s         # 1 vector\n",
    "    R0   = Tsexp(logx)           # 1 vector\n",
    "    R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "    R0  *= dtf                   # 1 vector\n",
    "    M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    v    = b.T.dot(M2).dot(b)\n",
    "    v    = T.clip(v,1e-3,5)      \n",
    "    vc   = 1+0.5*v\n",
    "    M1  += C*(R0*vc)             # Kx1 matrix (column) spiking noise\n",
    "    J    = Cb*R0+Adt             # KxK matrix\n",
    "    JM2  = J.dot(M2)             # KxK matrix\n",
    "    M2  += JM2 + JM2.T           # KxK matrix\n",
    "    M2  += CC*R0                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "\n",
    "oversample = 1\n",
    "mnr        = 0\n",
    "mxr        = 2\n",
    "mnlr       = -55\n",
    "mxlr       = log(maxrate)\n",
    "mnm        = -55\n",
    "mxm        = mxlr\n",
    "mnv        = 1e-6\n",
    "mxv        = 10\n",
    "dtf        = dt/oversample\n",
    "Adt        = Tcon(A*dtf)\n",
    "mnvc       = 1e-6\n",
    "mxvc       = 10\n",
    "\n",
    "def filter_2ND(y,s,nll,M1,M2): \n",
    "    ll = 0\n",
    "    dy = y/oversample\n",
    "    '''\n",
    "    # projected moments\n",
    "    m       = b.T.dot(M1)           # scalar\n",
    "    m       = T.clip(m,mnm,mxm)     # scalar\n",
    "    v       = b.T.dot(M2).dot(b)    # scalar\n",
    "    v       = T.clip(v,mnv,mxv)     # scalar\n",
    "    t       = Tsinv(v)              # scalar\n",
    "    # univariate measurement \n",
    "    # likelihood 0: integrated\n",
    "    mp,vp,l = intmoment(m,v,y,s,dtf)# scalar scalar scalar\n",
    "    mp      = T.clip(mp,mnm,mxm)\n",
    "    vp      = T.clip(vp,mnv,mxv)\n",
    "    tp      = Tsinv(vp)             # scalar\n",
    "    # log-rate and rate\n",
    "    lr      = m+s                   # 1 vector\n",
    "    lr      = T.clip(lr,mnlr,mxlr)  # 1 vector\n",
    "    lr     += T.log(dt)\n",
    "    R0      = Tsexp(lr)             # 1 vector\n",
    "    R0      = T.clip(R0,mnr,mxr)    # 1 vector\n",
    "    # variance correction\n",
    "    vc      = 1+0.5*v               # scalar\n",
    "    R1      = R0*vc                 # 1 vector\n",
    "    # likelihood 1: point\n",
    "    #l       = (y*lr-R0)[0]         # scalar\n",
    "    # likelihood 2: moment \n",
    "    #ll     = (y*lr-R1)[0]          # scalar\n",
    "    # likelihood 3: gaussian at posterior mean\n",
    "    # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "    #lr      = mp + s + T.log(dt)\n",
    "    #lpyx    = (y*lr-Tsexp(lr))\n",
    "    #dkl     =  0.5*(Tslog(v)-Tslog(vp)+Tsdiv((mp-m)**2,v))\n",
    "    lpxy    = -0.5*Tslog(2*pi*vp)\n",
    "    lpx     = -0.5*Tslog(2*pi*v) + -0.5*Tsdiv((m-mp)**2,vp)\n",
    "    #l       = (lpyx-dkl)[0]\n",
    "    # likelihood 4: gaussian at prior mean\n",
    "    lr      = m + s + T.log(dt)\n",
    "    lpyx    = (y*lr-Tsexp(lr))\n",
    "    lpx     = -0.5*Tslog(2*pi*v)\n",
    "    lpxy    = -0.5*Tslog(2*pi*vp) + -0.5*Tsdiv((m-mp)**2,v)\n",
    "    l       = (lpyx+lpx-lpxy)[0]\n",
    "    ll     += l\n",
    "    '''\n",
    "    for j in range(oversample):\n",
    "        # projected moments\n",
    "        m       = b.T.dot(M1)           # scalar\n",
    "        m       = T.clip(m,mnm,mxm)     # scalar\n",
    "        v       = b.T.dot(M2).dot(b)    # scalar\n",
    "        v       = T.clip(v,mnv,mxv)     # scalar\n",
    "        t       = Tsinv(v)              # scalar\n",
    "        # univariate measurement \n",
    "        # likelihood 0: integrated\n",
    "        mp,vp,l = intmoment(m,v,dy,s,dtf)# scalar scalar scalar\n",
    "        mp      = T.clip(mp,mnm,mxm)\n",
    "        vp      = T.clip(vp,mnv,mxv)\n",
    "        tp      = Tsinv(vp)             # scalar\n",
    "        # log-rate and rate\n",
    "        lr      = m+s                   # 1 vector\n",
    "        lr      = T.clip(lr,mnlr,mxlr)  # 1 vector\n",
    "        lr     += T.log(dtf)\n",
    "        R0      = Tsexp(lr)             # 1 vector\n",
    "        R0      = T.clip(R0,mnr*dtf,mxr*dtf) # 1 vector\n",
    "        # variance correction\n",
    "        vc      = exp(0.5*v)#1+0.5*v               # scalar\n",
    "        vc      = T.clip(vc,mnvc,mxvc)\n",
    "        R1      = R0*vc                 # 1 vector\n",
    "        R1      = T.clip(R1,mnr*dtf,mxr*dtf)\n",
    "        # likelihood 1: point\n",
    "        #l       = (dy*lr-R0)[0]         # scalar\n",
    "        # likelihood 2: moment \n",
    "        #l      = (dy*lr-R1)[0]          # scalar\n",
    "        # likelihood 3: gaussian at posterior mean\n",
    "        # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "        '''\n",
    "        lrp     = T.clip(mp+s,mnlr,mxlr) + T.log(dtf)\n",
    "        lpyx    = (dy*lrp-Tsexp(lrp))\n",
    "        lpxy    = -0.5*Tslog(2*pi*vp)\n",
    "        lpx     = -0.5*Tslog(2*pi*v) + -0.5*Tsdiv((m-mp)**2,v)\n",
    "        l       = (lpyx+lpx-lpxy)[0]\n",
    "        '''\n",
    "        # likelihood 4: gaussian at prior mean\n",
    "        # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "        lpyx    = (dy*lr-Tsexp(lr))\n",
    "        lpx     = -0.5*Tslog(2*pi*v)\n",
    "        lpxy    = -0.5*Tslog(2*pi*vp) + -0.5*Tsdiv((m-mp)**2,vp)\n",
    "        l       = (lpyx+lpx-lpxy)[0]\n",
    "        \n",
    "        # Another way to get R1?\n",
    "        #R1 = dy*lr-l\n",
    "        #R1 = T.clip(R1,mnr*dtf,mxr*dtf)\n",
    "        \n",
    "        ll     += l\n",
    "        # surrogate likelihood\n",
    "        tr      = nozero(tp-t)          # scalar\n",
    "        vr      = Tsinv(tr)             # scalar\n",
    "        mr      = (mp*tp-m*t)*vr        # scalar\n",
    "        mr      = T.clip(mr,mnm,mxm)\n",
    "        vr      = T.clip(vr,mnv,mxv)\n",
    "        # conditional MVG update\n",
    "        M2b     = M2.dot(beta.T)        # column\n",
    "        K       = Tsdiv(M2b,vr+v)       # column\n",
    "        M2      = M2-K.dot(M2b.T)       # matrix\n",
    "        M1      = M1+K*(mr-m)           # column \n",
    "        # moment updates\n",
    "        M1     += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1     += C*R1                  # Kx1 matrix (column) spiking noise\n",
    "        J       = Cb*R0+Adt             # KxK matrix\n",
    "        JM2     = J.dot(M2)             # KxK matrix\n",
    "        M2     += JM2 + JM2.T           # KxK matrix\n",
    "        M2     += CC*R1                 # KxK matrix\n",
    "    ll = ll*llgain+llbias               # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_2ND, allM1_2ND, allM2_2ND], up2ND = theano.scan(filter_2ND,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'filter_2ND')\n",
    "sumnll_2ND = cumnll_2ND[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_2ND = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_2ND,allM1_2ND,allM2_2ND],\n",
    "                  upd  = up2ND)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_2ND    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_2ND],\n",
    "                  upd  = up2ND)\n",
    "\n",
    "print('Second order filter with measurement defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p0.copy()\n",
    "p1[1:]*=0.5\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,p1,1,0)\n",
    "print(nll_2ND)\n",
    "subplot(211)\n",
    "lm = logmean(allM1_2ND,p1)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "p1 = p0.copy()\n",
    "p1[1:]*=0.5\n",
    "x0 = p1 + randn(*p1.shape)*1e-9\n",
    "result_2ND = x0\n",
    "\n",
    "#result_2ND =  [-5.43162804492,-571.708831348,-66.2875445888,0.701551340968,0.242166133938,-1.07866171916,0.913775086156,-1.93349055936,0.0018026120004,0.242277583707,-1.06062782385,1.69293329864,-0.645696170203,0.344691373584,-0.0862146637981,-0.00394170162424,0.000699250245375]\n",
    "\n",
    "print(result_2ND)\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "def objective_2ND(p):\n",
    "    nll = NLL_2ND(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        else:\n",
    "            nll = large\n",
    "    return nll\n",
    "\n",
    "print('Starting optimization')\n",
    "recalibrate_likelihood(NLL_2ND,grad_2ND,result_2ND)\n",
    "result_2ND = minimize_retry(objective_2ND,result_2ND,jac=False,verbose=verbose,simplex_only=True,tol=1e-12)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_2ND])+']')\n",
    "\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_2ND-p0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_2ND),lw=0.4,label=\"2ND-filter\")\n",
    "title('2ND fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,result_2ND,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_2ND),lw=0.4,label=\"2ND-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('2ND fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "# Sample from point-process models\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_2ND,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('2NDfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('2NDfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_2ND)).squeeze()\n",
    "print('Mean-field mean-rate for 2NDfilt is',mean(rate2))\n",
    "\n",
    "figure()\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('2NDfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(211)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,p0,1,0)\n",
    "lm = logmean(allM1_2ND,p1)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)\n",
    "\n",
    "subplot(212)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,result_2ND,1,0)\n",
    "lm = logmean(allM1_2ND,result_2ND)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,result_2ND)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Theano filter likelihood integrator\n",
    "\n",
    "We need to define a recursive filtering function, then we will try to get gradients via backpropagation through time. \n",
    "We cannot accurately backpropagatin through the non-conjugate measurement update, so we define two passes. The first pass computes surrogate Gaussian measurements, which are conjugate, and approximate the true Poisson measurements. The second pass uses these surrogate measurements, and (hopefully!) will be stable enough to get a gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: filter to compute surrogate measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "#dtype = 'float64'\n",
    "#Xst  = T.matrix(\"Xst\",dtype=dtype) # stimulus history features\n",
    "#Ysp  = T.vector(\"Ysp\",dtype=dtype) # spikes\n",
    "#par  = T.col(\"par\",dtype=dtype) # packed parameter vectors\n",
    "\n",
    "\n",
    "Cb  = C.dot(b.T) # if b is a vector, Cb will be a vector\n",
    "CC  = C.dot(C.T)\n",
    "meanrate     = np.mean(Y)\n",
    "rate_penalty = 500\n",
    "reg_cov      = 1e-6\n",
    "prior_prec   = 1e-8\n",
    "\n",
    "b    = par[1:K+1,:] # spike history weights\n",
    "mm   = par[0,0]\n",
    "\n",
    "# Pre-compute projected stimulus\n",
    "stim = mm + Xst.dot(par[K+1:,:])\n",
    "\n",
    "# Hard-coded parameters\n",
    "oversample   = 5\n",
    "dt           = T.constant(1      ,dtype=dtype)\n",
    "meanrate     = T.constant(np.mean(Y),dtype=dtype)\n",
    "\n",
    "maxlogr      = T.constant(1      ,dtype=dtype)\n",
    "maxrate      = T.constant(2      ,dtype=dtype)\n",
    "\n",
    "eps          = T.constant(np.finfo(dtype).eps  ,dtype=dtype)\n",
    "rate_penalty = T.constant(500    ,dtype=dtype)\n",
    "reg_cov      = T.constant(1e-6   ,dtype=dtype)\n",
    "prior_prec   = T.constant(1e-8   ,dtype=dtype)\n",
    "\n",
    "# Constants\n",
    "dtf = dt/oversample\n",
    "Cb  = C.dot(b.T)\n",
    "CC  = C.dot(C.T)\n",
    "Adt = A*dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_moment_theano(m,v,y,s,dt):\n",
    "    # Get moments by integrating\n",
    "    # Construct range for integration\n",
    "    m0,s0 = m,T.sqrt(v) \n",
    "    x = T.constant(np.linspace(-4,4,100),dtype='float32')*s0+m0\n",
    "    # Calculate likelihood contribution\n",
    "    lograte = x+s+Tslog(dt)\n",
    "    l  = y*(lograte)-Tsexp(lograte)\n",
    "    # Normalize to prevent overflow\n",
    "    offset = T.max(l)\n",
    "    l -= offset\n",
    "    # Calculate prior contribution\n",
    "    l += -0.5*(  Tsdiv((x-m)**2,v) + Tslog(v) + Tslog(2*pi) )\n",
    "    # Estimate posterior\n",
    "    p  = Tsexp(l)*Tsexp(offset)\n",
    "    py = nozero(T.sum(p))\n",
    "    p /= py\n",
    "    # Integrate to get posterior moments\n",
    "    m  = T.sum(x*p)\n",
    "    v  = T.sum((x-m)**2*p)\n",
    "    return m,v#,Tslog(py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_update(y,s,M1,M2):\n",
    "    '''\n",
    "    (mr,tr): `surrogate` Gaussian approximation to likelihood\n",
    "    '''\n",
    "    # Gaussian prior on log-rate\n",
    "    M2b    = M2.dot(b)\n",
    "    v      = nozero(b.T.dot(M2b))\n",
    "    m      = b.T.dot(M1)\n",
    "    t      = Tsinv(v)\n",
    "    # Regularizing Gaussian prior on log-rate\n",
    "    tq     = prior_prec + t\n",
    "    vq     = Tsinv(tq)\n",
    "    mq     = (m*t+mm*prior_prec)*vq\n",
    "    m,v,t  = mq,vq,tq\n",
    "    # Integrate to get posterior moments (cannot differentite this)\n",
    "    mp,vp  = update_moment_theano(m,v,y,s,dt)\n",
    "    tp     = Tsinv(vp)\n",
    "    # Generate surrogate univariate likelihood\n",
    "    tr     = nozero(tp-t)\n",
    "    vr     = Tsinv(tr)\n",
    "    mr     = (mp*tp-m*t)*vr\n",
    "    # Optimized Kalman update using surrogate univariate likelihood\n",
    "    K      = Tsdiv(M2b,vr+v)\n",
    "    M2     = M2-K.dot(M2b.T)\n",
    "    M1     = M1+K*(mr-m)\n",
    "    # Compute log-likelihood from univariate log-rate update\n",
    "    logr   = T.minimum(maxlogr,mp+s)\n",
    "    logPyx = y*logr-Tsexp(logr)\n",
    "    DklQP  = 0.5*(Tslog(v)-Tslog(vp)+Tsdiv((mp-m)**2,v))\n",
    "    ll     = logPyx #- DklQP\n",
    "    ll     = ll[0,0]\n",
    "    return M1,M2,ll,mr,tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_moments(M1,M2,s):\n",
    "    for j in range(oversample):\n",
    "        logv = b.T.dot(M2).dot(b)\n",
    "        logx = b.T.dot(M1)+s\n",
    "        R0   = Tsexp(logx)\n",
    "        R0   = T.minimum(maxrate,R0)\n",
    "        R0   = R0*dtf\n",
    "        Rm   = R0*(1+0.5*logv)\n",
    "        Rm   = T.minimum(maxrate,Rm)\n",
    "        M1  += Adt.dot(M1) +  C*Rm\n",
    "        J    = Cb*R0+Adt\n",
    "        M2  += J.dot(M2) + M2.dot(J.T) + CC*Rm\n",
    "    return M1,M2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial condition for moments\n",
    "M1 = T.zeros((K,1),dtype=dtype)\n",
    "M2 = T.eye(K,dtype=dtype)*eps\n",
    "\n",
    "def filterer(y,s,nll,M1,M2):\n",
    "    # Integrate\n",
    "    #M1,M2 = integrate_moments(M1,M2,s)\n",
    "    # Regularize\n",
    "    #M2 = 0.5*(M2+M2.T)+reg_cov*T.eye(K)\n",
    "    # Measurement update\n",
    "    #M1,M2,ll,mr,tr = measurement_update(y,s,M1,M2)\n",
    "    #ll,mr,tr = 1,1,1\n",
    "    #return nll-ll/float(N),M1,M2,mr,tr\n",
    "    return 1.,M1,M2,1.,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll,allM1,allM2,allmr,alltr], updates = theano.scan(filterer,\n",
    "                                            sequences     = [Ysp,stim],\n",
    "                                            outputs_info  = [\n",
    "                                                T.constant(0.0,dtype='float64'),\n",
    "                                                M1,M2,None,None],\n",
    "                                            non_sequences = [],\n",
    "                                            n_steps       = N,\n",
    "                                            name          = 'filterscanner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
