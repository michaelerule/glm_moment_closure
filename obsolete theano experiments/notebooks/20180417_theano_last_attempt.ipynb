{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import absolute_import\n",
    "from __future__ import with_statement\n",
    "from __future__ import division\n",
    "from __future__ import nested_scopes\n",
    "from __future__ import generators\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "# Load scipy/numpy/matplotlib\n",
    "from   scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "from   pylab import *\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from warnings import warn\n",
    "\n",
    "# Configure figure resolution\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'   ] = 100\n",
    "\n",
    "from izh       import * # Routines for sampling Izhikevich neurons\n",
    "from plot      import * # Misc. plotting routines\n",
    "from glm       import * # GLM fitting\n",
    "from arppglm   import * # Sampling and integration\n",
    "from utilities import * # Other utilities\n",
    "from arguments import * # Argument verification\n",
    "\n",
    "import os\n",
    "dtype='float64'\n",
    "flags = 'mode=FAST_RUN,device=gpu,floatX=%s'%dtype\n",
    "if dtype!='float64':\n",
    "    flags += ',warn_float64=warn'\n",
    "os.environ[\"THEANO_FLAGS\"] = flags\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano_arppglm import *\n",
    "\n",
    "print('Workspace Initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved GLM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#filename = 'saved_training_model.mat'\n",
    "filename = 'saved_training_model_badburster.mat'\n",
    "\n",
    "saved_training_model = scipy.io.loadmat(filename)\n",
    "K  = np.array(saved_training_model['K'],dtype=dtype)\n",
    "B  = np.array(saved_training_model['B'],dtype=dtype)\n",
    "By = np.array(saved_training_model['By'],dtype=dtype)\n",
    "Bh = np.array(saved_training_model['Bh'],dtype=dtype)\n",
    "A  = np.array(saved_training_model['A'],dtype=dtype)\n",
    "C  = np.array(saved_training_model['C'],dtype=dtype)\n",
    "Y  = asvector(np.array(saved_training_model['Y'],dtype=dtype))\n",
    "dt = np.array(saved_training_model['dt'],dtype=dtype)\n",
    "\n",
    "Bh_train = saved_training_model['Bh_train']\n",
    "By_train = saved_training_model['By_train']\n",
    "X_train  = concatenate([By_train,Bh_train],axis=1)\n",
    "Y_train  = asvector(saved_training_model['Y_train'])\n",
    "\n",
    "Bh_test  = saved_training_model['Bh_test']\n",
    "By_test  = saved_training_model['By_test']\n",
    "X_test   = concatenate([By_test,Bh_test],axis=1)\n",
    "Y_test   = asvector(saved_training_model['Y_test'])\n",
    " \n",
    "K  = int(scalar(K))\n",
    "N  = prod(Y.shape)\n",
    "\n",
    "'''# Don't use all training data\n",
    "STARTPLOT = 0#2000\n",
    "NPLOT = N#5000\n",
    "Y  = Y[STARTPLOT:STARTPLOT+NPLOT]\n",
    "By = By[STARTPLOT:STARTPLOT+NPLOT]\n",
    "Bh = Bh[STARTPLOT:STARTPLOT+NPLOT]\n",
    "X  = X[STARTPLOT:STARTPLOT+NPLOT]\n",
    "'''\n",
    "N = len(X_train)\n",
    "STARTPLOT=0\n",
    "NPLOT=N\n",
    "\n",
    "print('Saved GLM features loaded')\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STARTSHOW = 14000\n",
    "#STOPSHOW = 16000\n",
    "STARTSHOW = 0\n",
    "STOPSHOW = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def lograte(Bh,By,p):\n",
    "    '''\n",
    "    Log-intensity of point process model on this dataset\n",
    "    Predicted using the standard GLM way\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    lograte = m + Bh.dot(beta_st) + By.dot(beta)\n",
    "    return lograte\n",
    "\n",
    "def logmean(Bh,M1,p):\n",
    "    '''\n",
    "    Projected history process\n",
    "    Predicted using history-process means\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    M1      = np.squeeze(M1)\n",
    "    return (beta.T.dot(M1.T))[0] + (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "def get_stim(Bh,p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "    return stim\n",
    "\n",
    "def filter_GLM_np(Bh,p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = get_stim(Bh,p)\n",
    "    allM1_np = np.zeros((N,K))\n",
    "    M1       = np.zeros((K,1))\n",
    "    for i in range(N):\n",
    "        R   = scalar(sexp(p0[1:K+1].dot(M1)+m+stim[i]))\n",
    "        M1 += A.dot(M1)*dt + C.dot(R)\n",
    "        allM1_np[i] = M1[:,0]\n",
    "    return allM1_np\n",
    "\n",
    "def addspikes_(Y_=None):\n",
    "    if Y_ is None or Y_ is True:\n",
    "        Y_ = Y\n",
    "    for t in find(Y_>0):\n",
    "        axvline(t,color=OCHRE,lw=0.4)\n",
    "    \n",
    "def niceaxis(plotspikes=True):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\",message='No labelled objects found')\n",
    "        legend()\n",
    "    simpleraxis()\n",
    "    xlim(STARTSHOW,STOPSHOW)\n",
    "    if plotspikes is True or not plotspikes is None:\n",
    "        addspikes_(plotspikes)\n",
    "\n",
    "print('GLM helpers done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Re-fit GLM\n",
    "m,bhat  = fitGLM(X_train,asvector(Y_train))\n",
    "\n",
    "# Re-pack model parameters\n",
    "p0      = np.zeros((1+len(bhat)))\n",
    "p0[0 ]  = m\n",
    "p0[1:]  = bhat\n",
    "\n",
    "allM1_np = filter_GLM_np(Bh_train,p0)\n",
    "subplot(311)\n",
    "plot(lograte(Bh_train,By_train,p0),lw=0.4,label='conditional intensity')\n",
    "plot(logmean(Bh_train,allM1_np,p0),lw=0.4,label='mean-field',color=RUST)\n",
    "niceaxis()\n",
    "ylim(min(lograte(Bh_train,By_train,p0)),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters at which to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "oversample = 2    # Integration resolution\n",
    "maxrate    = 10.0 # Largest allowed rate\n",
    "maxvcorr   = 10.0 # Largest allowed variance correction\n",
    "dt         = 1.0  # Data time resolution\n",
    "reg_cov    = 1e-4\n",
    "reg_rate   = 1e-9\n",
    "\n",
    "p = p0.copy()\n",
    "#p[1:K+1] *= 0.775\n",
    "stim_np = get_stim(Bh_train,p)\n",
    "beta_np = ascolumn(p[1:K+1])\n",
    "print('Filtering using p=',v2str(p))\n",
    "\n",
    "# Helper function to compute negative expected log-likelihood\n",
    "def post_hoc_nll(LR,LV):\n",
    "    R0 = sexp(LR)\n",
    "    R1 = R0*(1+0.5*LV)\n",
    "    ELL  = np.mean(Y*LR - R1)\n",
    "    return -ELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Theano routines\n",
    "\n",
    "For integrating moments (not conditioned on data), filtering (conditioned on data), and filtering using surrogate likelihoods (Gaussian approximations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from theano_arppglm import *\n",
    "\n",
    "GLM_log_intensity, GLMNLL_f, GLMNLL_g, GLMNLL_h = build_ML_GLM_likelihood_theano()\n",
    "\n",
    "integrate_moments_theano, EMNLL_filt, EMNLL_grad = build_integrate_moments_theano(N,A,C,\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\")\n",
    "\n",
    "filter_moments_theano, NLL_filt, NLL_grad = build_filter_moments_theano(N,A,C,\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\",\n",
    "    measurement = \"moment\",\n",
    "    reg_cov     = reg_cov,\n",
    "    reg_rate    = reg_rate,\n",
    "    return_surrogates = True)\n",
    "\n",
    "filter_surrogate_theano, SNLL_filt, SNLL_grad = build_filter_moments_theano(N,A,C,\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\",\n",
    "    measurement = \"moment\",\n",
    "    reg_cov     = reg_cov,\n",
    "    reg_rate    = reg_rate,\n",
    "    return_surrogates = False,\n",
    "    use_surrogates    = True)\n",
    "\n",
    "print('Theano functions bulit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p0.copy()\n",
    "p[1:]*=0.25\n",
    "\n",
    "m        = array(p).ravel()[0]\n",
    "beta     = ascolumn(p[1:K+1])\n",
    "beta_st  = ascolumn(p[1+K:])\n",
    "stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "tic()\n",
    "allLRn,allLVn,allM1n,allM2n,nlln,mrn,vrn = filter_moments(stim,Y,A,beta,C,p[0],\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\",\n",
    "    measurement = \"moment\",\n",
    "    reg_cov     = reg_cov,\n",
    "    reg_rate    = reg_rate,\n",
    "    return_surrogates = True)\n",
    "toc()\n",
    "subplot(411)\n",
    "stderrplot(allLRn,allLVn,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,5)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, numpy')\n",
    "print('nll, numpy',nlln)\n",
    "\n",
    "tic()\n",
    "allLRt,allLVt,allM1t,allM2t,nllt,mrt,vrt = filter_moments_theano(Bh,Y,p)\n",
    "toc()\n",
    "subplot(412)\n",
    "stderrplot(allLRt,allLVt,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,5)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, theano')\n",
    "print('nll, theano',nllt)\n",
    "\n",
    "subplot(413)\n",
    "plot(allLRn,color=BLACK,label='log-λ numpy')\n",
    "plot(allLRt,':',color=RUST,label='log-λ theano')\n",
    "plot(allLRni,color=AZURE,label='log-λ no measurements')\n",
    "niceaxis()\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "#xlim(14900,15100)\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that Theano matches GLMfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "#### Optimization summary report helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_sample(Bh,p,M):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "    return ensemble_sample(stim,B,beta,M=100)\n",
    "\n",
    "def summarize_result(X,Y,Bh,result):\n",
    "    Y = asvector(Y)\n",
    "    \n",
    "    # Sample from point-process models\n",
    "    y1,l1 = get_sample(Bh,p0,1000)\n",
    "    y2,l2 = get_sample(Bh,result,1000)\n",
    "    print('True   mean rate is',mean(Y))\n",
    "    print('GLMfit mean rate is',mean(y1))\n",
    "    print('MCfilt mean rate is',mean(y2))\n",
    "    print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "    print('MCfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "    # FIGURE\n",
    "    figure(figsize=(10,8))\n",
    "    \n",
    "    subplot(411)\n",
    "    LM,LV,_,_ = integrate_moments_theano(Bh,result)\n",
    "    stderrplot(LM,LV,color=BLACK,lw=0.4)\n",
    "    niceaxis(Y)\n",
    "    title('Moment-closure density')\n",
    "    \n",
    "    subplot(412)\n",
    "    LM,LV,_,_,NLLS,_,_ = filter_moments_theano(Bh,Y,result)\n",
    "    stderrplot(LM,LV,color=BLACK,lw=0.4)\n",
    "    niceaxis(Y)\n",
    "    title('Conditional density')\n",
    "    NSAMP = 100\n",
    "    \n",
    "    subplot(413)\n",
    "    pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "    noaxis(); \n",
    "    xlabel('Time (ms)'); \n",
    "    ylabel('Sample',fontsize=9); \n",
    "    niceaxis(Y)\n",
    "    title('GLMfit')\n",
    "    \n",
    "    subplot(414)\n",
    "    pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "    noaxis(); \n",
    "    xlabel('Time (ms)'); \n",
    "    ylabel('Sample',fontsize=9); \n",
    "    niceaxis(Y)\n",
    "    title('MCfilt')\n",
    "    tight_layout()\n",
    "    \n",
    "    figure()\n",
    "    p = array(result)\n",
    "    m = p[0]\n",
    "    b = p[1:K+1]\n",
    "    h = p[K+1:]\n",
    "    b.shape,B.shape\n",
    "    \n",
    "    subplot(211)\n",
    "    l = X.dot(p[1:]) + p[0]\n",
    "    plot(l,color=BLACK,lw=1,label='MCfilt')\n",
    "    l = X.dot(p0[1:]) + p0[0]\n",
    "    plot(l,color=RUST,lw=1,label='GLMfit')\n",
    "    xlim(STARTSHOW,STOPSHOW)\n",
    "    simpleraxis()\n",
    "\n",
    "    subplot(212)\n",
    "    dB = log(10)*10\n",
    "    plot(b.dot(B)*dB)\n",
    "    plot(h.dot(B)*dB)\n",
    "    axhline(0,color='k',lw=0.5)\n",
    "    simpleraxis()\n",
    "    xlim(0,150)\n",
    "    tight_layout()\n",
    "\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial conditions and objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "GLM_result = p0 + randn(*np.shape(p0))*10\n",
    "GLM_result[1:]*=0.2\n",
    "GLM_result = p0*0\n",
    "\n",
    "large   = sqrt(np.finfo('float32').max)\n",
    "\n",
    "X,Y = X_train,Y_train\n",
    "\n",
    "def objective(p):\n",
    "    NLL = GLMNLL_f(X,Y,p)[0]\n",
    "    return NLL if isfinite(NLL) else large\n",
    "\n",
    "def gradient(p):\n",
    "    return GLMNLL_g(X,Y,p)[0]\n",
    "\n",
    "def hessian(p):\n",
    "    return GLMNLL_h(X,Y,p)[0]\n",
    "\n",
    "def og(p):\n",
    "    o = objective(p)\n",
    "    g = gradient(p)\n",
    "    return o,g\n",
    "\n",
    "# Check gradients\n",
    "g1 = gradient(GLM_result)\n",
    "g2 = numeric_grad(objective,GLM_result,10**-4)\n",
    "#print(g1)\n",
    "#print(g2)\n",
    "print('Mean absolute relative error in gradient:',mean(abs(g1-g2)/abs(g1)))\n",
    "\n",
    "# check hessian\n",
    "h1 = hessian(GLM_result)\n",
    "h2 = numeric_hess(gradient,GLM_result,1e-4)\n",
    "#print('Symbolic:\\n','\\n'.join(map(v2str,h1)))\n",
    "#print('Numeric:\\n','\\n'.join(map(v2str,h2)))\n",
    "#print('Difference:\\n','\\n'.join(map(v2str,h1-h2)))\n",
    "print('Mean absolute relative error in hessian:',mean(abs(h1-h2)/abs(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting optimization')\n",
    "try:\n",
    "    GLM_result = minimize_retry(og,GLM_result,jac=True,hess=hessian,verbose=True,options={'eps':1e-4},tol=1e-12)\n",
    "    GLM_result = minimize_retry(og,GLM_result,jac=True,verbose=True,options={'eps':1e-4},tol=1e-12)\n",
    "    GLM_result = minimize_retry(objective,GLM_result,jac=False,verbose=True,tol=1e-12,\n",
    "                           options={'eps':1e-4,'maxiter':1e19,'maxfev':1e19})\n",
    "    GLM_result = minimize_retry(objective,GLM_result,jac=False,verbose=True,simplex_only=True,tol=1e-12,\n",
    "                           options={'maxiter':inf,'maxfev':inf})\n",
    "    print(\"Finished optimization\")\n",
    "except KeyboardInterrupt:\n",
    "    print('Optimization paused')\n",
    "\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in GLM_result])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(GLM_result-p0)))\n",
    "\n",
    "summarize_result(X_train,asvector(Y_train),Bh_train,GLM_result)\n",
    "#GLM_log_intensity, GLMNLL_f, GLMNLL_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check moment integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "allLRni,allLVni,allM1ni,allM2ni = integrate_moments(stim_np,A,beta_np,C,\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\")\n",
    "toc()\n",
    "subplot(411)\n",
    "stderrplot(allLRni,allLVni,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-20,10)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Integrating, numpy')\n",
    "print('ENLL=',post_hoc_nll(allLRni,allLVni))\n",
    "\n",
    "tic()\n",
    "allLRti,allLVti,allM1ti,allM2ti = integrate_moments_theano(Bh,p0)\n",
    "toc()\n",
    "subplot(412)\n",
    "stderrplot(allLRti,allLVti,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-20,10)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Integrating, theano')\n",
    "print('ENLL=',post_hoc_nll(allLRti,allLVti))\n",
    "\n",
    "subplot(413)\n",
    "plot(allLRni,color=BLACK,label='log-λ, numpy')\n",
    "plot(allLRti,color=RUST,label='log-λ, theano')\n",
    "niceaxis()\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Log-intensity')\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize\n",
    "\n",
    "Here goes nothing. We want to combine conditional NLL with expected NLL. Expected NLL fits based on single time marginals of model response to stimuli, not conditioned on observed spikes. NLL conditionde on observed spikes, but may be stable even for unstable models and so is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "p1     = p0.copy()\n",
    "ELL_result = p1\n",
    "\n",
    "ELL_result = [-5.14182409466,1.11109383391,-4.82878604739,1.83673294557,1.79422798288,-3.48898983213,2.98998295676,-2.43428255153,0.71238421449,1.37600129637,-1.72296587502,2.17808722548,-1.31781555208,0.85589939253,-0.480755976108,0.24930797051,-0.0928260843248]\n",
    "ELL_result = [-5.3278358189,0.689932896008,-4.36386887719,2.59152547217,0.0190567168914,-2.27453018523,2.62021528673,-2.49639075347,0.901691011863,1.51751189519,-2.00478119387,2.38041164695,-1.35729594407,0.850552046267,-0.467364999388,0.238760435641,-0.0927765029538]\n",
    "ELL_result = [-5.10885924759,1.12824061962,-2.41714077891,1.01788191554,-0.00115025957431,-1.39257503717,1.21171230522,-1.6188025382,0.401108538778,0.507814976896,-0.987658945039,1.73799933077,-1.01917944975,0.65723363294,-0.34806237986,0.191621899837,-0.0747745444539]\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize only expected Log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large   = sqrt(np.finfo('float32').max)\n",
    "\n",
    "def objective(p):\n",
    "    # Expected negative log-likelihood\n",
    "    NLL = EMNLL_filt(Bh,Y,p)[0]\n",
    "    if not isfinite(NLL):\n",
    "        NLL = large\n",
    "    return NLL\n",
    "\n",
    "def gradient(p):\n",
    "    GNLL = EMNLL_grad(Bh,Y,p)[0]\n",
    "    return GNLL\n",
    "\n",
    "def og(p):\n",
    "    o = objective(p)\n",
    "    g = gradient(p)\n",
    "    return o,g\n",
    "\n",
    "print('Starting optimization')\n",
    "\n",
    "try:\n",
    "    ELL_result = minimize_retry(og,ELL_result,jac=True,verbose=True)\n",
    "    ELL_result = minimize_retry(objective,ELL_result,jac=False,verbose=True,simplex_only=True)\n",
    "    print(\"Finished optimization\")\n",
    "except KeyboardInterrupt:\n",
    "    print('Optimization paused')\n",
    "    \n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in ELL_result])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(ELL_result-p0)))\n",
    "\n",
    "summarize_result(X_train,Y_train,Bh_train,ELL_result)\n",
    "summarize_result(X_test,Y_test,Bh_test,ELL_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate filtering (conditional) likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ELL_result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "m        = array(p).ravel()[0]\n",
    "beta     = ascolumn(p[1:K+1])\n",
    "beta_st  = ascolumn(p[1+K:])\n",
    "stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "tic()\n",
    "allLRn,allLVn,allM1n,allM2n,nlln,mrn,vrn = filter_moments(stim,Y,A,beta,C,p[0],\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\",\n",
    "    measurement = \"moment\",\n",
    "    reg_cov     = reg_cov,\n",
    "    reg_rate    = reg_rate,\n",
    "    return_surrogates = True)\n",
    "toc()\n",
    "subplot(411)\n",
    "stderrplot(allLRn,allLVn,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,5)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, numpy')\n",
    "print('nll, numpy',nlln)\n",
    "\n",
    "tic()\n",
    "allLRt,allLVt,allM1t,allM2t,nllt,mrt,vrt = filter_moments_theano(Bh,Y,p)\n",
    "toc()\n",
    "subplot(412)\n",
    "stderrplot(allLRt,allLVt,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-15,5)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, theano')\n",
    "print('nll, theano',nllt)\n",
    "\n",
    "subplot(413)\n",
    "plot(allLRn,color=BLACK,label='log-λ numpy')\n",
    "plot(allLRt,':',color=RUST,label='log-λ theano')\n",
    "plot(allLRni,color=AZURE,label='log-λ no measurements')\n",
    "niceaxis()\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "#xlim(14900,15100)\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check filtering using surrogate measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "allLRns,allLVns,allM1ns,allM2ns,nllns = filter_moments(stim,Y,A,beta,C,p[0],\n",
    "    dt          = dt,\n",
    "    oversample  = oversample,\n",
    "    maxrate     = maxrate,\n",
    "    maxvcorr    = maxvcorr,\n",
    "    method      = \"second_order\",\n",
    "    int_method  = \"euler\",\n",
    "    measurement = \"moment\",\n",
    "    reg_cov     = reg_cov,\n",
    "    reg_rate    = reg_rate,\n",
    "    return_surrogates = False,\n",
    "    use_surrogates = (mrn,vrn))\n",
    "toc()\n",
    "subplot(411)\n",
    "stderrplot(allLRns,allLVns,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-20,10)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, numpy')\n",
    "print('nll, surrogate numpy',nllns)\n",
    "\n",
    "tic()\n",
    "allLRts,allLVts,allM1ts,allM2ts,nllts = filter_surrogate_theano(Bh,Y,p,mrt,vrt)\n",
    "toc()\n",
    "subplot(412)\n",
    "stderrplot(allLRt,allLVt,color=BLACK,lw=0.5)\n",
    "niceaxis()\n",
    "ylim(-20,10)\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "title('Filtering, theano')\n",
    "print('nll, surrogate theano',nllts)\n",
    "\n",
    "subplot(413)\n",
    "plot(mrn,color=BLACK,label='Surrogate mean updates, numpy')\n",
    "plot(mrt,color=RUST,label='Surrogate mean updates, theano')\n",
    "niceaxis()\n",
    "title('Surrogate means')\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "\n",
    "subplot(414)\n",
    "plot(allLRn,color=BLACK,label='Conditional log-λ, numpy')\n",
    "plot(allLRt,color=RUST,label='Conditional log-λ, theano')\n",
    "plot(allLRni,color=AZURE,label='Moment closure mean')\n",
    "plot(allLRns,color=CRIMSON,label='Surrogate log-λ, numpy')\n",
    "plot(allLRts,':',color=MAUVE,zorder=inf,label='Surrogate log-λ, theano')\n",
    "niceaxis()\n",
    "title('Using surrogate updates')\n",
    "xlim(STARTSHOW,STOPSHOW)\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize using ELL and conditional hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resut = (result+p0)*0.5\n",
    "#result = p0\n",
    "result = ELL_result#(ELL_result+GLM_result)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict  = False\n",
    "verbose = False\n",
    "large   = sqrt(np.finfo('float32').max)\n",
    "\n",
    "allLRt,allLVt,allM1t,allM2t,nllt,mrt,vrt = filter_moments_theano(Bh,Y,result)\n",
    "\n",
    "def objective(p):\n",
    "    NLL = SNLL_filt(Bh,Y,p,mrt,vrt)[0]\n",
    "    if not isfinite(NLL):\n",
    "        NLL = large\n",
    "    return NLL\n",
    "\n",
    "p = result\n",
    "print(SNLL_grad(Bh,Y,p,mrt,vrt)[0])\n",
    "from glm import numeric_grad\n",
    "print(numeric_grad(objective,p,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [-5.21847116314,0.454763857218,-0.726327441249,0.738281986681,-0.626254125845,0.494580375459,-0.327687909616,0.150363362699,-0.0282694097873,-4.71687411694,2.93672402134,0.294688287784,-0.536458963393,0.545767817347,-0.517988679391,0.430871438156,-0.320890257658]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ELL_result*0.5 + p*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(p):\n",
    "    NLLM = SNLL_filt(Bh,Y,p,mrt,vrt)[0]\n",
    "    #NLLP = GLMNLL_f(X,Y,p)[0]\n",
    "    #NLLQ = EMNLL_filt(X,Y,p)\n",
    "    NLL = NLLM #+ NLLP\n",
    "    #NLL -= 0.0880704571813\n",
    "    #NLL -= -2.39810128699e-09\n",
    "    #NLL -= -1.37653780625e-13 \n",
    "    return NLL if isfinite(NLL) else large\n",
    "\n",
    "def gradient(p):\n",
    "    GNLLM = SNLL_grad(Bh,Y,p,mrt,vrt)[0]\n",
    "    #GNLLP = GLMNLL_g(X,Y,p)[0]\n",
    "    #GEML = \n",
    "    #GENL = EMNLL_grad(Bh,Y,p)[0]\n",
    "    g = GNLLM #+ GNLLP\n",
    "    return g\n",
    "\n",
    "def og(p):\n",
    "    o = objective(p)\n",
    "    g = gradient(p)\n",
    "    return o,g\n",
    "    \n",
    "try:\n",
    "    #result = minimize_retry(og,result,jac=True,verbose=verbose,simplex_only=False,options={'eps':1e-3})\n",
    "    #result = minimize_retry(objective,result,jac=False,verbose=verbose,simplex_only=False,options={'eps':1e-3})\n",
    "    result = minimize_retry(objective,result,jac=False,verbose=verbose,simplex_only=True)\n",
    "    print(\"Finished optimization\")\n",
    "except KeyboardInterrupt:\n",
    "    print('Optimization paused')\n",
    "    \n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result-p0)))\n",
    "\n",
    "summarize_result(X_train,asvector(Y_train),Bh_train,result)\n",
    "summarize_result(X_test,asvector(Y_test),Bh_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_result(X_train,asvector(Y_train),Bh_train,result)\n",
    "summarize_result(X_test,asvector(Y_test),Bh_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
