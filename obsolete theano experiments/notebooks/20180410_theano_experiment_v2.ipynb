{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Load scipy/numpy/matplotlib\n",
    "from   scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "from   pylab import *\n",
    "\n",
    "# Configure figure resolution\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'   ] = 100\n",
    "\n",
    "from izh       import * # Routines for sampling Izhikevich neurons\n",
    "from plot      import * # Misc. plotting routines\n",
    "from glm       import * # GLM fitting\n",
    "from arppglm   import * # Sampling and integration\n",
    "from utilities import * # Other utilities\n",
    "from arguments import * # Argument verification\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "dtype = 'float64'\n",
    "\n",
    "import os\n",
    "flags = 'mode=FAST_RUN,device=gpu,floatX=%s'%dtype\n",
    "\n",
    "#flags = 'mode=fast_compile,device=gpu,floatX=%s'%dtype\n",
    "if dtype!='float64':\n",
    "    flags += ',warn_float64=warn'\n",
    "print(flags)\n",
    "os.environ[\"THEANO_FLAGS\"] = flags\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "from theano.compile.nanguardmode import NanGuardMode\n",
    "NANGUARD = NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=True)\n",
    "\n",
    "print('Workspace Initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Theano helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def Tcon(x):\n",
    "    return T.constant(x,dtype=dtype)\n",
    "\n",
    "eps     = 1e-4#Tcon(np.finfo('float32').eps)\n",
    "max_exp = Tcon(4)#Tcon(np.log(np.sqrt(np.finfo('float32').max)))\n",
    "\n",
    "def nozero(x):\n",
    "    '''Clip number to be larger than `eps`'''\n",
    "    return T.maximum(eps,x)\n",
    "    #return T.log(1+T.exp(x*10))/10\n",
    "\n",
    "def Tslog(x):\n",
    "    '''Theano safe logarithm'''\n",
    "    return T.log(nozero(x))\n",
    "\n",
    "def Tsexp(x):\n",
    "    return T.exp(T.minimum(max_exp,x))\n",
    "\n",
    "def Tsinv(x):\n",
    "    return 1.0/nozero(x)\n",
    "\n",
    "def Tsdiv(a,x):\n",
    "    return a/nozero(x)\n",
    "\n",
    "def Tfun(inp=None,out=None,upd=None):\n",
    "    return theano.function(inputs               = inp,\n",
    "                           outputs              = out,\n",
    "                           updates              = upd,\n",
    "                           on_unused_input      = 'warn',\n",
    "                           allow_input_downcast = True)\n",
    "#,\n",
    " #                          mode                 = NANGUARD)\n",
    "    \n",
    "print('Theano helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved features for GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "saved_training_model = scipy.io.loadmat('saved_training_model.mat')\n",
    "K  = np.array(saved_training_model['K'],dtype=dtype)\n",
    "B  = np.array(saved_training_model['B'],dtype=dtype)\n",
    "By = np.array(saved_training_model['By'],dtype=dtype)\n",
    "Bh = np.array(saved_training_model['Bh'],dtype=dtype)\n",
    "A  = np.array(saved_training_model['A'],dtype=dtype)\n",
    "C  = np.array(saved_training_model['C'],dtype=dtype)\n",
    "Y  = np.array(saved_training_model['Y'],dtype=dtype)\n",
    "dt = np.array(saved_training_model['dt'],dtype=dtype)\n",
    "\n",
    "K  = int(scalar(K))\n",
    "N  = prod(Y.shape)\n",
    "Y  = np.squeeze(Y)\n",
    "X  = concatenate([By,Bh],axis=1)\n",
    "\n",
    "# Don't use all training data\n",
    "'''N  = 3000\n",
    "Y  = Y[:N]\n",
    "By = By[:N]\n",
    "Bh = Bh[:N]\n",
    "X  = X[:N]'''\n",
    "\n",
    "STARTPLOT = 2000\n",
    "NPLOT = 5000\n",
    "Y  = Y[STARTPLOT:STARTPLOT+NPLOT]\n",
    "By = By[STARTPLOT:STARTPLOT+NPLOT]\n",
    "Bh = Bh[STARTPLOT:STARTPLOT+NPLOT]\n",
    "X  = X[STARTPLOT:STARTPLOT+NPLOT]\n",
    "\n",
    "N = len(X)\n",
    "STARTPLOT=0\n",
    "NPLOT=N\n",
    "\n",
    "print('Saved GLM features loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GLM on CPU and verify that filtering approximates basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def lograte(p):\n",
    "    '''\n",
    "    Log-intensity of point process model on this dataset\n",
    "    Predicted using the standard GLM way\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    lograte = m + Bh.dot(beta_st) + By.dot(beta)\n",
    "    return lograte\n",
    "\n",
    "def logmean(M1,p):\n",
    "    '''\n",
    "    Projected history process\n",
    "    Predicted using history-process means\n",
    "    '''\n",
    "    m       = array(p).ravel()[0]\n",
    "    beta    = ascolumn(p[1:K+1])\n",
    "    beta_st = ascolumn(p[1+K:])\n",
    "    M1      = np.squeeze(M1)\n",
    "    return (beta.T.dot(M1.T))[0] + (m + Bh.dot(beta_st))[:,0]\n",
    "\n",
    "def filter_GLM_np(p):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim     = (m + Bh.dot(beta_st))[:,0]\n",
    "    allM1_np = np.zeros((N,K))\n",
    "    M1       = np.zeros((K,1))\n",
    "    for i in range(N):\n",
    "        R   = sexp(p0[1:K+1].dot(M1)+m+stim[i])\n",
    "        M1 += A.dot(M1)*dt + C.dot(R)\n",
    "        allM1_np[i] = M1[:,0]\n",
    "    return allM1_np\n",
    "\n",
    "def addspikes():\n",
    "    for t in find(Y>0):\n",
    "        axvline(t,color=OCHRE,lw=0.4)\n",
    "    \n",
    "def niceaxis():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\",message='No labelled objects found')\n",
    "        legend()\n",
    "    simpleraxis()\n",
    "    xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "    addspikes()\n",
    "\n",
    "print('GLM helpers done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Re-fit GLM\n",
    "m,bhat  = fitGLM(X,Y)\n",
    "\n",
    "# Re-pack model parameters\n",
    "p0      = np.zeros((1+len(bhat)))\n",
    "p0[0 ]  = m\n",
    "p0[1:]  = bhat\n",
    "\n",
    "allM1_np = filter_GLM_np(p0)\n",
    "\n",
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,label='conditional intensity')\n",
    "subplot(311)\n",
    "plot(logmean(allM1_np,p0),lw=0.4,label='mean-field',color=RUST)\n",
    "niceaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Fit GLM on Theano using mean-field\n",
    "\n",
    "This will neglect conditional structure available in the data and focus on matching the slow-timescales with the mean-field limit of the GLM\n",
    "\n",
    "We should adjust the negative-log-likelihood for best machine precision. It should be rescaled so that the initial guess has negative-log-likelihood of zero, and so that the gradient is steep enough so as not to suffer from precision loss. We will implement this with a scalar offset and gain for the negative log-likelihood, passed as parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mean-field filtering equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "Xst  = T.matrix(\"Xst\",dtype=dtype) # stimulus history features\n",
    "Ysp  = T.vector(\"Ysp\",dtype=dtype) # spikes\n",
    "par  = T.vector(\"par\",dtype=dtype) # packed parameter vectors\n",
    "\n",
    "# Negative log-likelihood gain and offset to\n",
    "# mitigate precision loss\n",
    "llgain = T.scalar('llgain',dtype=dtype)\n",
    "llbias = T.scalar('llbias',dtype=dtype)\n",
    "\n",
    "# Cast A to theano consatn\n",
    "Aop = Tcon(A)\n",
    "Cop = Tcon(C)\n",
    "\n",
    "# Unpack parameter vector\n",
    "b    = par[1:K+1] # spike history weights\n",
    "bst  = par[K+1:]  # stimulus weights\n",
    "mm   = par[0]     # constant offset\n",
    "\n",
    "# Pre-compute projected stimulus\n",
    "# This evaluates to a vector\n",
    "stim = mm + Xst.dot(bst)\n",
    "\n",
    "# Hard-coded parameters\n",
    "oversample   = 5\n",
    "dt           = 1.0\n",
    "maxrate      = Tcon(2)\n",
    "maxlogr      = Tslog(maxrate)\n",
    "\n",
    "# Constants\n",
    "dtf = dt/oversample\n",
    "Adt = Aop*dtf\n",
    "\n",
    "# Initial condition for moments\n",
    "# These should be theano variables as they will \n",
    "# need to be passedto initialize the scan function\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*eps\n",
    "\n",
    "def GLM_ll(y,s,M1):                  # scalar, scalar, column\n",
    "    m      = b.T.dot(M1)             # b:vector, M1:column, result:len-1 vector\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    ll     = y*logr-Tsexp(logr)      # 1 vector\n",
    "    return ll[0]                     # scalar\n",
    "\n",
    "def integrate_GLM(M1,s):             # column, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = Tsexp(logx)           # 1 vector\n",
    "        R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "        M1  += Cop*(R0*dtf)          # Kx1 matrix (column) spiking noise\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    return M1\n",
    "    \n",
    "def GLM_filter(y,s,nll,M1):          # scalar, scalar, scalar, column\n",
    "    M1 = integrate_GLM(M1,s)         # Kx1 matrix (column)\n",
    "    ll = GLM_ll(y,s,M1)              # scalar\n",
    "    ll = ll*llgain+llbias            # scalar\n",
    "    return nll-ll,M1                 # scalar, column\n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_GLM, allM1_GLM], updates_GLM = theano.scan(GLM_filter,\n",
    "                                            sequences     = [Ysp,stim],\n",
    "                                            outputs_info  = [Tcon(0),M1],\n",
    "                                            non_sequences = [],\n",
    "                                            n_steps       = N,\n",
    "                                            name          = 'GLM_updates')\n",
    "\n",
    "sumnll_GLM     = cumnll_GLM[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_GLM = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out = [sumnll_GLM,allM1_GLM],\n",
    "                  upd = updates_GLM)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_GLM    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out = [sumnll_GLM],\n",
    "                  upd = updates_GLM)\n",
    "\n",
    "print('Theano GLM mean-field fitting routines defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for MFfilt GLM\n",
    "\n",
    "These aren't very useful and have been disabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll_gain = 1\n",
    "ll_bias = 0\n",
    "from glm import numeric_grad, numeric_hess\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grad_GLM = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                out = [theano.gradient.jacobian(sumnll_GLM,par)],\n",
    "                upd = updates_GLM)\n",
    "print(grad_GLM)\n",
    "\n",
    "'''\n",
    "# Hessian takes way too long to build\n",
    "hess_GLM = Tfun(inp = [Xst,Ysp,par], \n",
    "                out = [theano.gradient.hessian(sumnll_GLM,par)],\n",
    "                upd = updates_GLM)\n",
    "print(hess_GLM)\n",
    "'''\n",
    "\n",
    "print('(gradients done)')\n",
    "\n",
    "### Check numeric gradients\n",
    "print(grad_GLM(Bh,Y,p0,ll_gain,ll_bias)[0])\n",
    "print(numeric_grad(lambda x:NLL_GLM(Bh,Y,x,ll_gain,ll_bias),p0))#,np.finfo('float32').eps))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate suitable log-likelihood gain and bias for best precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def recalibrate_likelihood(NLL_fun,grad_fun,p,verbose=False):\n",
    "    global ll_gain, ll_bias\n",
    "    ll_gain = 1\n",
    "    ll_bias = 0\n",
    "    maxgain = 100\n",
    "    normalize_gain_to = sqrt(N)\n",
    "    nll0   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not isfinite(nll0):\n",
    "        warn('Likelihood is not finite, cannot rescale!')\n",
    "        return\n",
    "    if verbose: print('initial nll',nll0)\n",
    "    if not grad_fun is None:\n",
    "        gr0    = grad_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "        if not all(isfinite(gr0)):\n",
    "            warn('Gradient is not finite, cannot rescale!')\n",
    "            return\n",
    "        rmsgr0 = max(1e-6,sqrt(mean(gr0**2)))\n",
    "        if verbose: print('initial root mean squared gradient',rmsgr0)\n",
    "        ll_gain = min(maxgain,normalize_gain_to/rmsgr0)\n",
    "        if verbose: print('ll_gain',ll_gain)\n",
    "        gr1    = grad_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "        if not all(isfinite(gr1)):\n",
    "            warn('Gradient is not finite, cannot rescale!')\n",
    "            return\n",
    "        rmsgr1 = max(1e-6,sqrt(mean(gr1**2)))\n",
    "        print('rescaled RMS gradient',rmsgr1)\n",
    "    nll1   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if not isfinite(nll1):\n",
    "        warn('Likelihood is not finite, cannot rescale!')\n",
    "        return\n",
    "    if verbose: print('rescaled nll',nll1)\n",
    "    ll_bias = nll1/N\n",
    "    for i in range(5):\n",
    "        nll2   = NLL_fun(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "        if not isfinite(nll2):\n",
    "            warn('Likelihood is not finite, cannot rescale!')\n",
    "            return\n",
    "        ll_bias += nll2/N\n",
    "    if verbose: print('ll_bias',ll_bias)\n",
    "    print('Shifted nll is ',nll2)\n",
    "\n",
    "recalibrate_likelihood(NLL_GLM,grad_GLM,p0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize GLM on Theano using mean-field filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "result = array(p0).copy()\n",
    "\n",
    "# Begin at previous best guess for this result\n",
    "# result = [-5.17943358722,-26.4301822764,-9.09410987003,9.08387276285,0.472761995485,-2.36125927326,1.31525541432,-1.60600566038,0.0284241341053,0.847363810973,-1.03972611426,1.94527757845,-0.81379858301,0.785027995141,-0.234973129833,-0.103747652871,-0.00791078822163]\n",
    "#result = [-6.45553910662,11.2856391467,-20.3654371892,3.87616060703,-1.88432240077,-2.61043347539,-1.62103766004,0.33936732957,-1.91357319581,1.27403649416,-1.21780969965,2.28911681412,-0.619505705252,1.13827148037,0.0377996712583,-0.31138730526,-0.0608561896186]\n",
    "\n",
    "#result = [-6.79133649847,-29.2860814902,-3.30612421806,-0.906768019782,1.99314064172,-10.2237003882,13.1065486065,\n",
    "#          -11.3484098455,0.361443691536,8.3253484781,-5.50982057309,2.49346127861,0.4618367441,0.00426349208114,\n",
    "#          -0.393253366568,0.512513212603,-0.199505838674]\n",
    "\n",
    "result = [-6.48913576754,-43.636990799,-5.48686452302,0.784435684979,-5.24260149358,-10.2759276512,13.6851897803,-12.8003504345,-3.54568724789,10.7072811055,-6.37363364666,3.69099435923,-1.05639579671,2.31080008374,-1.84169083341,1.26295152888,-0.32236353414]\n",
    "\n",
    "print(result)\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from plot import v2str, v2str_long\n",
    "\n",
    "verbose   = False\n",
    "tolerance = 1e-9\n",
    "maxiter   = 1000\n",
    "maxfev    = 1000\n",
    "\n",
    "def objective_GLM(p):\n",
    "    nll = NLL_GLM(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        raise ArithmeticError('Invalid likelihood')\n",
    "    return nll\n",
    "\n",
    "print(\"Beginning optimization\")\n",
    "recalibrate_likelihood(NLL_GLM,grad_GLM,result)\n",
    "result = minimize_retry(objective_GLM,result,jac=None,verbose=verbose,failthrough=True,\n",
    "                        options={'maxiter':100,'maxfev':100},tol=tolerance,simplex_only=True)\n",
    "print(\"Finished optimization\")\n",
    "\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that mean-field fitting is close to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result),lw=0.4,label=\"MF-filter\")\n",
    "title('Mean-field fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "NLL,allM1 = filter_GLM(Bh,Y,result,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result),lw=0.4,label=\"MF-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('Mean-field fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "\n",
    "def get_sample(p,M=100):\n",
    "    m        = array(p).ravel()[0]\n",
    "    beta     = ascolumn(p[1:K+1])\n",
    "    beta_st  = ascolumn(p[1+K:])\n",
    "    stim_np  = (m + Bh.dot(beta_st))[:,0]\n",
    "    y,l = ensemble_sample(stim_np,B,beta,M=M)\n",
    "    return y,l\n",
    "\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('MFfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('MFfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result)).squeeze()\n",
    "print('Mean-field mean-rate for MFfilt is',mean(rate2))\n",
    "\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('MFfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize GLM using LNA without measurement updates\n",
    "\n",
    "Incorporate estimate of fluctuations into likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field definitions)\n",
    "# Limit variance corrections\n",
    "maxvcorr = 100\n",
    "\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "\n",
    "reg_1 = ateb.dot(T.eye(1)*1e-6).dot(ateb)\n",
    "reg_2 = T.eye(K)*1e-6\n",
    "reg_C = reg_2 + reg_2\n",
    "\n",
    "# Additional constants for covariance evolution\n",
    "Cop = Tcon(C)                        # Matrix\n",
    "Cb  = Cop.dot(beta)                  # Matrix\n",
    "CC  = Cop.dot(Cop.T)                 # Matrix\n",
    "\n",
    "def LNA_ll(y,s,M1,M2):               # scalar, scalar, column, matrix\n",
    "    m      = b.T.dot(M1)             # b:vector, M1:column, result:len-1 vector\n",
    "    v      = b.T.dot(M2).dot(b)      # scalar\n",
    "    v      = nozero(v)\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    vc     = 1+0.5*v\n",
    "    vc     = T.minimum(maxvcorr,vc)  # scalar\n",
    "    R1     = R0*vc                   # 1 vector\n",
    "    ll     = y*logr-R1               # 1 vector\n",
    "    return ll[0]                     # scalar\n",
    "\n",
    "def integrate_LNA(M1,M2,s):          # column, matrix, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = Tsexp(logx)           # 1 vector\n",
    "        R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "        R0  *= dtf                   # 1 vector\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1  += C*(R0)                # Kx1 matrix (column) spiking noise\n",
    "        J    = Cb*R0+Adt             # KxK matrix\n",
    "        JM2  = J.dot(M2)             # KxK matrix\n",
    "        M2  += JM2 + JM2.T           # KxK matrix\n",
    "        M2  += CC*R0                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "def LNA_filter(y,s,nll,M1,M2):       # scalar, scalar, scalar, column, matrix\n",
    "    M1,M2 = integrate_LNA(M1,M2,s)   # Kx1 matrix (column)\n",
    "    ll    = LNA_ll(y,s,M1,M2)        # scalar\n",
    "    ll    = ll*llgain+llbias         # scalar\n",
    "    return nll-ll,M1,M2     # scalar, column, matrix\n",
    "\n",
    "# Initial condition for second moment\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_LNA, allM1_LNA, allM2_LNA], upLNA = theano.scan(LNA_filter,\n",
    "                                                sequences     = [Ysp,stim],\n",
    "                                                outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                non_sequences = [],\n",
    "                                                n_steps       = N,\n",
    "                                                name          = 'LNA_filter')\n",
    "sumnll_LNA = cumnll_LNA[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_LNA = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out = [sumnll_LNA,allM1_LNA, allM2_LNA],\n",
    "                  upd = upLNA)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_LNA    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out = [sumnll_LNA],\n",
    "                  upd = upLNA)\n",
    "\n",
    "print('Defined LNA filtering GLM likelihood (no measurements)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate LNA without measurement updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def logvar(allM1,allM2,pp):\n",
    "    m       = array(pp).ravel()[0]\n",
    "    beta    = ascolumn(pp[1:K+1])\n",
    "    beta_st = ascolumn(pp[1+K:])\n",
    "    return np.array([beta.T.dot(m2).dot(beta) for m2 in allM2]).ravel()\n",
    "\n",
    "subplot(311)\n",
    "nll ,allM1, allM2 = filter_LNA(Bh,Y,p0,ll_gain,ll_bias)\n",
    "lm = logmean(allM1,p0)\n",
    "lv = logvar(allM1,allM2,p0)\n",
    "stderrplot(lm,lv,lw=0.5,label='LNA')\n",
    "niceaxis()\n",
    "ylim(-20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize GLM using LNA without measurement updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "x0 = p0 + randn(*p0.shape)*1e-4\n",
    "result_LNA = x0\n",
    "\n",
    "'''result_LNA = np.float128(['-5.26722577475', '-0.958476224094', '0.105001143274', '-1.3878277659', '-0.0530694634475', \n",
    "              '-1.15022127732', '-0.365875934645', '-0.0831215026589', '-0.50788907261', '-0.27615738965', \n",
    "              '0.721112129138', '-0.108877379667', '0.40652935799',  '-0.0195945097271', '0.174251749772', \n",
    "              '-0.161673313388', '-0.00141694765251'])\n",
    "'''\n",
    "\n",
    "result_LNA = [-5.7632510371,-0.694558984913,-0.365826544528,-4.80686708703,7.4369467517,-9.42028908679,7.119563688,-4.36419864785,0.734860845532,4.10351996955,-4.63602850103,3.91815160103,-1.97827248286,1.0536266355,-0.408518056414,0.141022673197,-0.0571683297285]\n",
    "\n",
    "result_LNA = [-5.9082880366,-0.73193407386,-0.368048019254,-4.72408820063,7.43576560468,-9.46939441666,6.9198757086,-4.36632236796,0.668945083848,4.15128851375,-4.63347162332,3.90207045538,-1.95639635235,1.05582847329,-0.404315514029,0.148024458989,-0.0594086298297]\n",
    "\n",
    "result_LNA = [-5.75926275329,-1.93504555723,0.389645810086,-4.06103323968,4.80955049052,-6.43743677629,4.35937131174,-3.21838662035,0.159402568812,4.44417794461,-4.48033625618,3.5044055644,-1.60090301317,0.993904308549,-0.544158672297,0.303170618085,-0.118989985737]\n",
    "\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "tolerance = 1e-9\n",
    "maxiter   = 1000\n",
    "maxfev    = 1000\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "\n",
    "def objective_LNA(p):\n",
    "    nll = NLL_LNA(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        else:\n",
    "            nll = large\n",
    "    return nll\n",
    "\n",
    "print('Starting optimization')\n",
    "recalibrate_likelihood(NLL_LNA,None,result_LNA)\n",
    "result_LNA = minimize_retry(objective_LNA,result_LNA,jac=False,verbose=verbose,simplex_only=True)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LNA])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LNA-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that LNA filtering regression is close to GLMfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_LNA),lw=0.4,label=\"LNA-filter\")\n",
    "title('LNA fitted conditional intensity')\n",
    "niceaxis()\n",
    "\n",
    "subplot(312)\n",
    "NLL,allM1 = filter_GLM(Bh,Y,result_LNA,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_LNA),lw=0.4,label=\"LNA-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('LNA fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_LNA,1000)\n",
    "\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('LNAfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('LNAfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_LNA)).squeeze()\n",
    "print('Mean-field mean-rate for LNAfilt is',mean(rate2))\n",
    "\n",
    "figure()\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('LNAfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "nll ,allM1, allM2 = filter_LNA(Bh,Y,result_LNA,ll_gain,ll_bias)\n",
    "lm = logmean(allM1,p0)\n",
    "lv = logvar(allM1,allM2,p0)\n",
    "stderrplot(lm,lv,lw=0.5,label='LNA')\n",
    "niceaxis()\n",
    "ylim(-20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate measurement updates into LNA filtering\n",
    "\n",
    "So far, we have only been computing the likelihood against single-time marginals from mean-field and LNA approximations to the point process. This neglects autocorrelations to some extent. Measurement updates are the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow and safe (ish) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "M2   = Tcon(np.eye(K)*1e-7)\n",
    "Adt  = Tcon(A*dt)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "oversample = 1\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "maxlogr = 2\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,50),dtype=dtype) # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*(Tsdiv((x-m)**2,v)+Tslog(v*2*pi)) # vector\n",
    "    lograte = x+s+Tslog(dt)                          # vector\n",
    "    lograte = T.maximum(maxlogr,lograte)             # vector\n",
    "    logPyx  = y*lograte-Tsexp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*Tsexp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = Tsexp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "def gaussian_measurement(y,s,M1,M2): # scalar scalar column matrix\n",
    "    m        = b.T.dot(M1)           # scalar\n",
    "    m        = T.maximum(-30,T.minimum(0,m))\n",
    "    v        = b.T.dot(M2).dot(b)    # scalar\n",
    "    v        = nozero(v)\n",
    "    v        = T.minimum(5,v)\n",
    "    t        = Tsinv(v)              # scalar\n",
    "    mp,vp,ll = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    mp       = T.maximum(-50,T.minimum(0,mp))\n",
    "    vp       = T.minimum(5,vp)\n",
    "    tp       = Tsinv(vp)             # scalar\n",
    "    tr       = nozero(tp-t)          # scalar\n",
    "    vr       = Tsinv(tr)             # scalar\n",
    "    mr       = (mp*tp-m*t)*vr        # scalar\n",
    "    M2b      = M2.dot(beta.T)        # column\n",
    "    K        = Tsdiv(M2b,vr+v)       # column\n",
    "    M2       = M2-K.dot(M2b.T)       # matrix\n",
    "    M1       = M1+K*(mr-m)           # column \n",
    "    # likelihood P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "    lr      = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    lpyx    = (y*lr-T.exp(lr))\n",
    "    lpx     = -0.5*T.log(2*pi*v)\n",
    "    lpxy    = -0.5*T.log(2*pi*vp) + -0.5*(m-mp)**2/vp\n",
    "    ll      = (lpyx+lpx-lpxy)[0]\n",
    "    return M1,M2,ll                  # column matrix scalar\n",
    "\n",
    "def integrate_LNA2(M1,M2,s):          # column, matrix, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = Tsexp(logx)           # 1 vector\n",
    "        R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "        R0  *= dtf                   # 1 vector\n",
    "        v    = b.T.dot(M2).dot(b)    # scalar\n",
    "        v    = nozero(v)\n",
    "        v    = T.minimum(Tcon(5),v)\n",
    "        vc   = exp(0.5*v)#1.0+0.5*v\n",
    "        vc   = T.minimum(maxvcorr,vc)  # scalar\n",
    "        R1   = R0*vc                   # 1 vector\n",
    "        R0   = R1\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1  += C*(R1)                # Kx1 matrix (column) spiking noise\n",
    "        J    = Cb*R1+Adt             # KxK matrix\n",
    "        JM2  = J.dot(M2)             # KxK matrix\n",
    "        M2  += JM2 + JM2.T           # KxK matrix\n",
    "        M2  += CC*R1                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "def LNA_filter2(y,s,nll,M1,M2):   \n",
    "    M1,M2    = integrate_LNA2(M1,M2,s)\n",
    "    M1,M2,ll = gaussian_measurement(y,s,M1,M2)\n",
    "    ll = ll*llgain+llbias            # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_LNA2, allM1_LNA2, allM2_LNA2], upLNA2 = theano.scan(LNA_filter2,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'LNA_filter2')\n",
    "sumnll_LNA2 = cumnll_LNA2[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_LNA2 = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_LNA2,allM1_LNA2,allM2_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_LNA2    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "\n",
    "print('LNA filter with measurement defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast and unsafe version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "M2   = Tcon(np.eye(K)*1e-7)\n",
    "Adt  = Tcon(A*dt)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "oversample = 1\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "maxlogr = 2\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,50),dtype=dtype) # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*((x-m)**2/v+T.log(v*2*pi)) # vector\n",
    "    lograte = x+s+T.log(dt)                          # vector\n",
    "    logPyx  = y*lograte-T.exp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*T.exp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = T.exp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "def gaussian_measurement(y,s,M1,M2): # scalar scalar column matrix\n",
    "    m        = b.T.dot(M1)           # scalar\n",
    "    v        = b.T.dot(M2).dot(b)    # scalar\n",
    "    v        = nozero(v)\n",
    "    t        = 1/v                   # scalar\n",
    "    mp,vp,ll = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    tp       = 1/vp                  # scalar\n",
    "    tr       = tp-t                  # scalar\n",
    "    vr       = 1/tr                  # scalar\n",
    "    mr       = (mp*tp-m*t)*vr        # scalar\n",
    "    M2b      = M2.dot(beta.T)        # column\n",
    "    K        = M2b/(vr+v)            # column\n",
    "    M2       = M2-K.dot(M2b.T)       # matrix\n",
    "    M1       = M1+K*(mr-m)           # column \n",
    "    # likelihood   P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "    lr      = m+s\n",
    "    lpyx    = (y*lr-T.exp(lr))\n",
    "    lpx     = -0.5*T.log(2*pi*v)\n",
    "    lpxy    = -0.5*T.log(2*pi*vp) + -0.5*(m-mp)**2/vp\n",
    "    ll      = (lpyx+lpx-lpxy)[0]\n",
    "    return M1,M2,ll                  # column matrix scalar\n",
    "\n",
    "def integrate_LNA2(M1,M2,s):          # column, matrix, scalar\n",
    "    for j in range(oversample):\n",
    "        logx = b.T.dot(M1)+s         # 1 vector\n",
    "        R0   = T.exp(logx)           # 1 vector\n",
    "        R0  *= dtf                   # 1 vector\n",
    "        v    = b.T.dot(M2).dot(b)    # scalar\n",
    "        v    = nozero(v)\n",
    "        vc   = 1.0+0.5*v#T.exp(0.5*v)\n",
    "        R1   = R0*vc                 # 1 vector\n",
    "        R0   = R1\n",
    "        M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1  += C*(R1)                # Kx1 matrix (column) spiking noise\n",
    "        J    = Cb*R0+Adt             # KxK matrix\n",
    "        JM2  = J.dot(M2)             # KxK matrix\n",
    "        M2  += JM2 + JM2.T           # KxK matrix\n",
    "        M2  += CC*R1                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "def LNA_filter2(y,s,nll,M1,M2):   \n",
    "    M1,M2    = integrate_LNA2(M1,M2,s)\n",
    "    M1,M2,ll = gaussian_measurement(y,s,M1,M2)\n",
    "    ll = ll*llgain+llbias            # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_LNA2, allM1_LNA2, allM2_LNA2], upLNA2 = theano.scan(LNA_filter2,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'LNA_filter2')\n",
    "sumnll_LNA2 = cumnll_LNA2[-1]\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_LNA2 = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_LNA2,allM1_LNA2,allM2_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "# GLM negative log-likelihood\n",
    "NLL_LNA2    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_LNA2],\n",
    "                  upd  = upLNA2)\n",
    "print('LNA filter with measurement defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the LNA filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "p1 = p0.copy()\n",
    "p1[1:]*=0.25\n",
    "\n",
    "x0 = p1 + randn(*p1.shape)*1e-9\n",
    "result_LNA2 = x0\n",
    "print(result_LNA2)\n",
    "print('Set initial conditions')\n",
    "\n",
    "nll_LNA2,allM1_LNA2,allM2_LNA2 = filter_LNA2(Bh,Y,x0,1,0)\n",
    "print(nll_LNA2)\n",
    "subplot(211)\n",
    "lm = logmean(allM1_LNA2,p1)\n",
    "lv = logvar (allM1_LNA2,allM2_LNA2,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "def objective_LNA2(p):\n",
    "    nll = NLL_LNA2(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        nll = large\n",
    "    return nll\n",
    "print('Starting optimization')\n",
    "recalibrate_likelihood(NLL_LNA2,None,result_LNA2)\n",
    "result_LNA2 = minimize_retry(objective_LNA2,result_LNA2,jac=False,verbose=verbose,simplex_only=True)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_LNA2])+']')\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_LNA2-p0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "subplot(311)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_LNA2),lw=0.4,label=\"LNA-filter\")\n",
    "title('LNA fitted conditional intensity')\n",
    "niceaxis()\n",
    "subplot(312)\n",
    "nll_LNA2,allM1_LNA2,allM2_LNA2 = filter_LNA2(Bh,Y,result_LNA2,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_LNA2),lw=0.4,label=\"LNA-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('LNA fitted mean-field rate')\n",
    "niceaxis()\n",
    "tight_layout()\n",
    "\n",
    "# Print statistics\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_LNA2,1000)\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('LNAfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('LNAfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_LNA2)).squeeze()\n",
    "print('Mean-field mean-rate for LNAfilt is',mean(rate2))\n",
    "\n",
    "figure()\n",
    "NSAMP = 50\n",
    "subplot(411)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(412)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('LNAfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-order moment closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,10),dtype='float32') # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*(Tsdiv((x-m)**2,v)+Tslog(v*2*pi)) # vector\n",
    "    lograte = x+s+Tslog(dt)                          # vector\n",
    "    lograte = T.maximum(maxlogr,lograte)             # vector\n",
    "    logPyx  = y*lograte-Tsexp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*Tsexp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = Tsexp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "def gaussian_measurement(y,s,M1,M2,dt): # scalar scalar column matrix\n",
    "    m        = b.T.dot(M1)           # scalar\n",
    "    m        = T.clip(m,-30,0)       # scalar\n",
    "    v        = b.T.dot(M2).dot(b)    # scalar\n",
    "    v        = T.clip(v,1e-3,5)      # scalar\n",
    "    t        = Tsinv(v)              # scalar\n",
    "    mp,vp,ll = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    mp       = T.clip(mp,-30,0)\n",
    "    vp       = T.clip(vp,1e-3,5)\n",
    "    tp       = Tsinv(vp)             # scalar\n",
    "    tr       = nozero(tp-t)          # scalar\n",
    "    vr       = Tsinv(tr)             # scalar\n",
    "    mr       = (mp*tp-m*t)*vr        # scalar\n",
    "    M2b      = M2.dot(beta.T)        # column\n",
    "    K        = Tsdiv(M2b,vr+v)       # column\n",
    "    M2       = M2-K.dot(M2b.T)       # matrix\n",
    "    M1       = M1+K*(mr-m)           # column \n",
    "    '''\n",
    "    # try this instead?\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    vc     = 1+0.5*v\n",
    "    vc     = T.minimum(maxvcorr,vc)  # scalar\n",
    "    R1     = R0*vc                   # 1 vector\n",
    "    ll     = (y*logr-R1)[0]          # scalar\n",
    "    '''\n",
    "    # Or just this?\n",
    "    logr   = T.minimum(maxlogr,m+s)  # 1 vector\n",
    "    R0     = Tsexp(logr)             # 1 vector\n",
    "    ll     = (y*logr-R0)[0]          # scalar\n",
    "    \n",
    "    return M1,M2,ll                  # column matrix scalar\n",
    "\n",
    "def integrate_2ND(M1,M2,s):          # column, matrix, scalar\n",
    "    logx = b.T.dot(M1)+s         # 1 vector\n",
    "    R0   = Tsexp(logx)           # 1 vector\n",
    "    R0   = T.minimum(maxrate,R0) # 1 vector\n",
    "    R0  *= dtf                   # 1 vector\n",
    "    M1  += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    v    = b.T.dot(M2).dot(b)\n",
    "    v    = T.clip(v,1e-3,5)      \n",
    "    vc   = 1+0.5*v\n",
    "    M1  += C*(R0*vc)             # Kx1 matrix (column) spiking noise\n",
    "    J    = Cb*R0+Adt             # KxK matrix\n",
    "    JM2  = J.dot(M2)             # KxK matrix\n",
    "    M2  += JM2 + JM2.T           # KxK matrix\n",
    "    M2  += CC*R0                 # KxK matrix\n",
    "    return M1,M2                     # column, matrix\n",
    "\n",
    "\n",
    "oversample = 1\n",
    "mnr        = 0\n",
    "mxr        = 2\n",
    "mnlr       = -100\n",
    "mxlr       = log(maxrate)\n",
    "mnm        = -100\n",
    "mxm        = mxlr\n",
    "mnv        = 1e-9\n",
    "mxv        = 100\n",
    "dtf        = dt/oversample\n",
    "Adt        = Tcon(A*dtf)\n",
    "mnvc       = 1e-9\n",
    "mxvc       = 100\n",
    "\n",
    "def filter_2ND(y,s,nll,M1,M2): \n",
    "    ll = 0\n",
    "    dy = y/oversample\n",
    "    '''\n",
    "    # projected moments\n",
    "    m       = b.T.dot(M1)           # scalar\n",
    "    m       = T.clip(m,mnm,mxm)     # scalar\n",
    "    v       = b.T.dot(M2).dot(b)    # scalar\n",
    "    v       = T.clip(v,mnv,mxv)     # scalar\n",
    "    t       = Tsinv(v)              # scalar\n",
    "    # univariate measurement \n",
    "    # likelihood 0: integrated\n",
    "    mp,vp,l = intmoment(m,v,y,s,dtf)# scalar scalar scalar\n",
    "    mp      = T.clip(mp,mnm,mxm)\n",
    "    vp      = T.clip(vp,mnv,mxv)\n",
    "    tp      = Tsinv(vp)             # scalar\n",
    "    # log-rate and rate\n",
    "    lr      = m+s                   # 1 vector\n",
    "    lr      = T.clip(lr,mnlr,mxlr)  # 1 vector\n",
    "    lr     += T.log(dt)\n",
    "    R0      = Tsexp(lr)             # 1 vector\n",
    "    R0      = T.clip(R0,mnr,mxr)    # 1 vector\n",
    "    # variance correction\n",
    "    vc      = 1+0.5*v               # scalar\n",
    "    R1      = R0*vc                 # 1 vector\n",
    "    # likelihood 1: point\n",
    "    #l       = (y*lr-R0)[0]         # scalar\n",
    "    # likelihood 2: moment \n",
    "    #ll     = (y*lr-R1)[0]          # scalar\n",
    "    # likelihood 3: gaussian at posterior mean\n",
    "    # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "    #lr      = mp + s + T.log(dt)\n",
    "    #lpyx    = (y*lr-Tsexp(lr))\n",
    "    #dkl     =  0.5*(Tslog(v)-Tslog(vp)+Tsdiv((mp-m)**2,v))\n",
    "    lpxy    = -0.5*Tslog(2*pi*vp)\n",
    "    lpx     = -0.5*Tslog(2*pi*v) + -0.5*Tsdiv((m-mp)**2,vp)\n",
    "    #l       = (lpyx-dkl)[0]\n",
    "    # likelihood 4: gaussian at prior mean\n",
    "    lr      = m + s + T.log(dt)\n",
    "    lpyx    = (y*lr-Tsexp(lr))\n",
    "    lpx     = -0.5*Tslog(2*pi*v)\n",
    "    lpxy    = -0.5*Tslog(2*pi*vp) + -0.5*Tsdiv((m-mp)**2,v)\n",
    "    l       = (lpyx+lpx-lpxy)[0]\n",
    "    ll     += l\n",
    "    '''\n",
    "    for j in range(oversample):\n",
    "        # projected moments\n",
    "        m       = b.T.dot(M1)           # scalar\n",
    "        #m       = T.clip(m,mnm,mxm)     # scalar\n",
    "        v       = b.T.dot(M2).dot(b)    # scalar\n",
    "        #v       = T.clip(v,mnv,mxv)     # scalar\n",
    "        t       = 1/v#Tsinv(v)              # scalar\n",
    "        # univariate measurement \n",
    "        # likelihood 0: integrated expected\n",
    "        mp,vp,l = intmoment(m,v,dy,s,dtf)# scalar scalar scalar\n",
    "        #mp      = T.clip(mp,mnm,mxm)\n",
    "        #vp      = T.clip(vp,mnv,mxv)\n",
    "        tp      = 1/vp#Tsinv(vp)             # scalar\n",
    "        # log-rate and rate\n",
    "        lr      = m+s                   # 1 vector\n",
    "        #lr      = T.clip(lr,mnlr,mxlr)  # 1 vector\n",
    "        lr     += T.log(dtf)\n",
    "        R0      = T.exp(lr)             # 1 vector\n",
    "        #R0      = T.clip(R0,mnr*dtf,mxr*dtf) # 1 vector\n",
    "        # variance correction\n",
    "        vc      = exp(0.5*v)#1+0.5*v               # scalar\n",
    "        #vc      = T.clip(vc,mnvc,mxvc)\n",
    "        R1      = R0*vc                 # 1 vector\n",
    "        #R1      = T.clip(R1,mnr*dtf,mxr*dtf)\n",
    "        # likelihood 1: point\n",
    "        #l       = (dy*lr-R0)[0]         # scalar\n",
    "        # likelihood 2: moment \n",
    "        #l      = (dy*lr-R1)[0]          # scalar\n",
    "        # likelihood 3: gaussian at posterior mean\n",
    "        # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "        '''\n",
    "        lrp     = T.clip(mp+s,mnlr,mxlr) + T.log(dtf)\n",
    "        lpyx    = (dy*lrp-Tsexp(lrp))\n",
    "        lpxy    = -0.5*Tslog(2*pi*vp)\n",
    "        lpx     = -0.5*Tslog(2*pi*v) + -0.5*Tsdiv((m-mp)**2,v)\n",
    "        l       = (lpyx+lpx-lpxy)[0]\n",
    "        '''\n",
    "        # likelihood 4: gaussian at prior mean\n",
    "        # P(y) ~ P(y|x)*P(x)/P(x|y)\n",
    "        lpyx    = (dy*lr-T.exp(lr))\n",
    "        lpx     = -0.5*T.log(2*pi*v)\n",
    "        lpxy    = -0.5*T.log(2*pi*vp) + -0.5*(m-mp)**2/vp\n",
    "        l       = (lpyx+lpx-lpxy)[0]\n",
    "        \n",
    "        # Another way to get R1?\n",
    "        #R1 = dy*lr-l\n",
    "        #R1 = T.clip(R1,mnr*dtf,mxr*dtf)\n",
    "        \n",
    "        ll     += l\n",
    "        # surrogate likelihood\n",
    "        tr      = tp-t#nozero(tp-t)          # scalar\n",
    "        vr      = 1/tr#Tsinv(tr)             # scalar\n",
    "        mr      = (mp*tp-m*t)*vr        # scalar\n",
    "        #mr      = T.clip(mr,mnm,mxm)\n",
    "        #vr      = T.clip(vr,mnv,mxv)\n",
    "        # conditional MVG update\n",
    "        M2b     = M2.dot(beta.T)        # column\n",
    "        K       = M2b/(vr+v)#sdiv(M2b,vr+v)       # column\n",
    "        M2      = M2-K.dot(M2b.T)       # matrix\n",
    "        M1      = M1+K*(mr-m)           # column \n",
    "        # moment updates\n",
    "        M1     += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "        M1     += C*R1                  # Kx1 matrix (column) spiking noise\n",
    "        J       = Cb*R0+Adt             # KxK matrix\n",
    "        JM2     = J.dot(M2)             # KxK matrix\n",
    "        M2     += JM2 + JM2.T           # KxK matrix\n",
    "        M2     += CC*R1                 # KxK matrix\n",
    "    ll = ll*llgain+llbias               # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_2ND, allM1_2ND, allM2_2ND], up2ND = theano.scan(filter_2ND,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'filter_2ND')\n",
    "sumnll_2ND = cumnll_2ND[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_2ND = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_2ND,allM1_2ND,allM2_2ND],\n",
    "                  upd  = up2ND)\n",
    "\n",
    "# GLM negative log-likelihood\n",
    "NLL_2ND    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_2ND],\n",
    "                  upd  = up2ND)\n",
    "\n",
    "print('Second order filter with measurement defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast, unsafe version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.slin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep input arguments, patameres, constants from the mean-field and LNA definitions)\n",
    "beta = b.dimshuffle(['x',0])          # column\n",
    "ateb = beta.T/(beta.T.dot(beta)[0,0]) # row\n",
    "Cop  = Tcon(C)          # Matrix\n",
    "Cb   = Cop.dot(beta)    # Matrix\n",
    "CC   = Tcon(C.dot(C.T)) # Matrix\n",
    "\n",
    "# Initial condition for moments\n",
    "M1 = Tcon(np.zeros((K,1)))\n",
    "M2 = T.eye(K,dtype=dtype)*1e-7\n",
    "\n",
    "# Integration range\n",
    "irange = T.constant(np.linspace(-3,3,10),dtype='float32') # vector\n",
    "\n",
    "def intmoment(m,v,y,s,dt): # scalar scalar scalar scalar scalar\n",
    "    m0,s0   = m,T.sqrt(v)                            # scalar scalar\n",
    "    x       = irange*s0 + m0                         # vector\n",
    "    logPx   = -0.5*(Tsdiv((x-m)**2,v)+Tslog(v*2*pi)) # vector\n",
    "    lograte = x+s+Tslog(dt)                          # vector\n",
    "    lograte = T.maximum(maxlogr,lograte)             # vector\n",
    "    logPyx  = y*lograte-Tsexp(lograte)               # vector\n",
    "    ll      = T.sum(logPyx*Tsexp(logPx))             # scalar\n",
    "    logPyx -= T.max(logPyx)                          # vector\n",
    "    Pxy     = Tsexp(logPyx+logPx)                    # vector\n",
    "    norm    = T.sum(nozero(Pxy))                     # scalar\n",
    "    m       = T.sum(x*Pxy)/norm                      # scalar\n",
    "    v       = T.sum((x-m)**2*Pxy)/norm               # scalar\n",
    "    return m,v,ll                                    # scalar scalar scalar\n",
    "\n",
    "Adt        = Tcon(A*dt)\n",
    "\n",
    "def filter_2ND(y,s,nll,M1,M2): \n",
    "    # projected moments\n",
    "    m       = b.T.dot(M1)           # scalar\n",
    "    v       = b.T.dot(M2).dot(b)    # scalar\n",
    "    v       = T.maximum(0,v)\n",
    "    t       = 1/v                   # scalar\n",
    "    # univariate measurement \n",
    "    mp,vp,l = intmoment(m,v,y,s,dt) # scalar scalar scalar\n",
    "    tp      = 1/vp                  # scalar\n",
    "    # log-rate and rate\n",
    "    lr      = m+s+T.log(dt)           # 1 vector\n",
    "    R0      = T.exp(lr)               # 1 vector\n",
    "    # variance correction\n",
    "    vc      = T.exp(0.5*v)#1+0.5*v    # scalar\n",
    "    R1      = R0*vc                 # 1 vector\n",
    "    # likelihood P(y)~P(y|x)*P(x)/P(x|y)\n",
    "    #lpyx    = (y*lr-T.exp(lr))\n",
    "    #lpx     = -0.5*T.log(2*pi*v)\n",
    "    #lpxy    = -0.5*T.log(2*pi*vp)+-0.5*(m-mp)**2/vp\n",
    "    #l       = (lpyx+lpx-lpxy)[0]\n",
    "    \n",
    "    l        = (y*lr - R1)[0]\n",
    "    # surrogate likelihood\n",
    "    tr      = tp-t                  # scalar\n",
    "    vr      = 1/tr                  # scalar\n",
    "    mr      = (mp*tp-m*t)*vr        # scalar\n",
    "    # conditional MVG update\n",
    "    M2b     = M2.dot(beta.T)        # column\n",
    "    K       = M2b/(vr+v)            # column\n",
    "    M2      = M2-K.dot(M2b.T)       # matrix\n",
    "    M1      = M1+K*(mr-m)           # column \n",
    "    # moment updates\n",
    "    M1     += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    M1     += C*R1                  # Kx1 matrix (column) spiking noise\n",
    "    J       = Cb*R0+Adt             # KxK matrix\n",
    "    JM2     = J.dot(M2)             # KxK matrix\n",
    "    M2     += JM2 + JM2.T           # KxK matrix\n",
    "    M2     += CC*R1                 # KxK matrix\n",
    "    ll = l*llgain+llbias            # scalar\n",
    "    return nll-ll,M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[cumnll_2ND, allM1_2ND, allM2_2ND], up2ND = theano.scan(filter_2ND,\n",
    "                                                    sequences     = [Ysp,stim],\n",
    "                                                    outputs_info  = [Tcon(0),M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'filter_2ND')\n",
    "sumnll_2ND = cumnll_2ND[-1]\n",
    "\n",
    "# GLM negative log-likelihood and history means\n",
    "filter_2ND = Tfun(inp = [Xst,Ysp,par,llgain,llbias],\n",
    "                  out  = [sumnll_2ND,allM1_2ND,allM2_2ND],\n",
    "                  upd  = up2ND)\n",
    "# GLM negative log-likelihood\n",
    "NLL_2ND    = Tfun(inp = [Xst,Ysp,par,llgain,llbias], \n",
    "                  out  = [sumnll_2ND],\n",
    "                  upd  = up2ND)\n",
    "\n",
    "def int_2ND(s,M1,M2): \n",
    "    m       = b.T.dot(M1)           # scalar\n",
    "    v       = b.T.dot(M2).dot(b)    # scalar\n",
    "    v       = T.maximum(0,v)\n",
    "    lr      = m+s+T.log(dt)         # 1 vector\n",
    "    R0      = T.exp(lr)             # 1 vector\n",
    "    vc      = T.exp(0.5*v)#1+0.5*v               # scalar\n",
    "    R1      = R0*vc                 # 1 vector\n",
    "    M1     += Adt.dot(M1)           # Kx1 matrix (column) deterministic\n",
    "    M1     += C*R1                  # Kx1 matrix (column) spiking noise\n",
    "    J       = Cb*R0+Adt             # KxK matrix\n",
    "    JM2     = J.dot(M2)             # KxK matrix\n",
    "    M2     += JM2 + JM2.T           # KxK matrix\n",
    "    M2     += CC*R1                 # KxK matrix\n",
    "    return M1,M2  \n",
    "\n",
    "# Scan to compute moments, likelihood, measurement approximations\n",
    "[iallM1_2ND, iallM2_2ND], iup2ND = theano.scan(int_2ND,\n",
    "                                                    sequences     = [stim],\n",
    "                                                    outputs_info  = [M1,M2],\n",
    "                                                    non_sequences = [],\n",
    "                                                    n_steps       = N,\n",
    "                                                    name          = 'int_2ND')\n",
    "\n",
    "integrate_2ND = Tfun(inp = [Xst,par],\n",
    "                  out  = [iallM1_2ND,iallM2_2ND],\n",
    "                  upd  = iup2ND)\n",
    "\n",
    "print('Second order filter with measurement defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p0.copy()\n",
    "p1[1:]*=0.25\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,p1,1,0)\n",
    "print(nll_2ND)\n",
    "subplot(211)\n",
    "lm = logmean(allM1_2ND,p1)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "x0 = p1 + randn(*p1.shape)*1e-9\n",
    "result_2ND = x0\n",
    "print(result_2ND)\n",
    "print('Set initial conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "strict    = False \n",
    "verbose   = False\n",
    "large     = sqrt(np.finfo('float32').max)\n",
    "def objective_2ND(p):\n",
    "    nll = NLL_2ND(Bh,Y,p,ll_gain,ll_bias)[0]\n",
    "    if verbose:\n",
    "        print('x=',v2str_long(p))\n",
    "        print('nll =',np.float128(nll).astype(str))\n",
    "    if not isfinite(nll):\n",
    "        if strict:\n",
    "            raise ArithmeticError('Invalid likelihood')\n",
    "        else:\n",
    "            nll = large\n",
    "    return nll\n",
    "\n",
    "print('Starting optimization')\n",
    "recalibrate_likelihood(NLL_2ND,grad_2ND,result_2ND)\n",
    "result_2ND = minimize_retry(objective_2ND,result_2ND,jac=False,verbose=verbose,simplex_only=True,tol=1e-12)\n",
    "print(\"Finished optimization\")\n",
    "print('x=','['+','.join([np.float128(x).astype(str) for x in result_2ND])+']')\n",
    "\n",
    "print(\"Total absolute change from GLM fit is\",sum(abs(result_2ND-p0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "subplot(411)\n",
    "plot(lograte(p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "plot(lograte(result_2ND),lw=0.4,label=\"2ND-filter\")\n",
    "title('2ND fitted conditional intensity')\n",
    "niceaxis()\n",
    "subplot(412)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,result_2ND,ll_gain,ll_bias)\n",
    "plot(logmean(allM1,result_2ND),lw=0.4,label=\"2ND-filter\")\n",
    "plot(logmean(allM1_np,p0),lw=0.4,color=RUST,label='GLMfit')\n",
    "title('2ND fitted mean-field rate')\n",
    "niceaxis()\n",
    "\n",
    "# Sample from point-process models\n",
    "y1,l1 = get_sample(p0,1000)\n",
    "y2,l2 = get_sample(result_2ND,1000)\n",
    "print('True   mean rate is',mean(Y))\n",
    "print('GLMfit mean rate is',mean(y1))\n",
    "print('2NDfilt mean rate is',mean(y2))\n",
    "print('GLMfit mean log-likelihood is',mean(Y[:,None]*l1 - sexp(l1)))\n",
    "print('2NDfilt mean log-likelihood is',mean(Y[:,None]*l2 - sexp(l2)))\n",
    "# This is higher than it should be because we didn't correct for self-inhibition?\n",
    "# Sample from original model\n",
    "rate1 = exp(logmean(allM1_np,p0)).squeeze()\n",
    "print('Mean-field mean-rate for GLMfit is',mean(rate1))\n",
    "# This is lower than it should be becaus we didn't correct for self-excitation\n",
    "# Sample from MF-filt fit model\n",
    "rate2 = exp(logmean(allM1,result_2ND)).squeeze()\n",
    "print('Mean-field mean-rate for 2NDfilt is',mean(rate2))\n",
    "\n",
    "NSAMP = 50\n",
    "subplot(413)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "subplot(414)\n",
    "pcolormesh(-int32(y2[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('2NDfilt')\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "subplot(411)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,p0,1,0)\n",
    "lm = logmean(allM1_2ND,p1)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,p1)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)\n",
    "subplot(412)\n",
    "nll_2ND,allM1_2ND,allM2_2ND = filter_2ND(Bh,Y,result_2ND,1,0)\n",
    "lm = logmean(allM1_2ND,result_2ND)\n",
    "lv = logvar (allM1_2ND,allM2_2ND,result_2ND)\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)\n",
    "\n",
    "subplot(413)\n",
    "allM1_2ND,allM2_2ND = integrate_2ND(Bh,p0)\n",
    "lm = logmean(allM1_2ND,p1)\n",
    "lv = maximum(0,logvar (allM1_2ND,allM2_2ND,p1))\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)\n",
    "\n",
    "subplot(414)\n",
    "allM1_2ND,allM2_2ND = integrate_2ND(Bh,result_2ND)\n",
    "lm = logmean(allM1_2ND,result_2ND)\n",
    "lv = maximum(0,logvar (allM1_2ND,allM2_2ND,result_2ND))\n",
    "stderrplot(lm,lv,lw=0.25)\n",
    "niceaxis()\n",
    "ylim(-20,0)\n",
    "tight_layout()\n",
    "\n",
    "figure()\n",
    "'''\n",
    "subplot(413)\n",
    "pcolormesh(-int32(y1[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "niceaxis()\n",
    "title('GLMfit')\n",
    "'''\n",
    "subplot(413)\n",
    "\n",
    "\n",
    "rate3 = exp(lm+0.5*lv)#*(1+0.5*lv)\n",
    "y3 = np.random.poisson(rate3[:,None],(len(rate3),NSAMP))\n",
    "print('Moment only mean rate is',mean(y3))\n",
    "pcolormesh(-int32(y3[:,:NSAMP].T>0),cmap='gray')\n",
    "noaxis(); \n",
    "xticks(arange(0,1001,100)); \n",
    "xlabel('Time (ms)'); \n",
    "ylabel('Sample',fontsize=9); \n",
    "xlim(STARTPLOT,STARTPLOT+NPLOT)\n",
    "niceaxis()\n",
    "title('2NDfilt')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
